---
title: "Startup Success Prediction"
subtitle: "Programming Languages - Project by Helena Schick"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_depth: 3
    number_sections: true
---

# Plan
## Identify Use Case

The objective of this project is to predict the success of startups. A startup is a new company founded by an entrepreneur with the intent to grow beyond the solo founder according to [wikipedia](https://en.wikipedia.org/wiki/Startup_company). 
Startups play major role in the economy, as they foster innovation and create employment as they grow. Yet, they face high uncertainty and need to find investors to continue their ideas and expand their potential. This project focuses on the investors to find companies with great potential to invest in and thus being one step ahead of the competition.
A business model canvas is used to describe the use case in more detail.

1. Customer Segments:
The costumers of this use case are investors that want to investigate the startup landscape quickly and want to make the best possible investment. 
The investors' biggest pain is the uncertainty of the investment in a startup. Even with a well-developed business plan and promising market research, there is no guarantee that the startup will be successful. Furthermore, startups are inherently risky ventures. They often have limited operating history and may face significant challenges in scaling up their operations, acquiring customers, and generating revenue. Moreover, investing in a startup requires a significant time commitment. Investors need to research potential opportunities, evaluate business plans, and actively monitor their investments. 
Their gains include high returns in the case that the startup will be successful. Especially early-stage investors have the opportunity to purchase equity at a low valuation, which can result in a significant return on investment if the company is acquired or goes public. In addition, investing in startups can help diversify an investor's portfolio. Startups can provide exposure to new market segments that may be difficult to access through traditional investments. Furthermore, some investors may be motivated by the social impact that startups can have. By investing in innovative companies that are addressing social or environmental challenges, investors can support positive change while also generating financial returns.

2. Value Proposition:
This projects addresses all of these pain points by providing detailed analysis of many variables former startups to transfer these learnings to new investments. It highlights the most important features of a startup to become successful and predicts whether a startup which is currently operating turns into a success or a failure. The success of a company is defined as the event that gives the company's founders a large sum of money through the process of M&A (Merger and Acquisition) or an IPO (Initial Public Offering). A company would be considered as failed if it had to be shut down.
This minimizes the uncertainty and risk of the investment. As this is provided in one platform, the time commitment and intense research is reduced tremendously. The data can always be extended through more startup data provided by the customers. This has the advantage that possible changes in the successful features are encountered and the use case always provides the latest and most valuable information.

3. Channels:
Specific investors will be contacted directly to learn about this startup success prediction and how to purchase it. In addition, the use case will be advertised on business platforms and online newspapers.

4. Customer Relationships:
A close customer collaboration will be created, that the investors have the information about startup success prediction and also share information about their past investments that the data is always improving and up to date.

5. Revenue Streams:
The startup success prediction can be purchased as a yearly abonnement by investors. They can get a reduction of the fee in exchange for startup investment data that can be incorporated in the analysis and prediction.

6. Key Activities:
The most important activity is always having a well optimized classification model for the startup success prediction. Another key activity is the data analysis that is used as a base for the prediction model. The third activity is to create a good user experience to visualize the key learnings and recommendations.

7. Key Resources:
The project is conducted by Helena Schick.
Jan Kirenz can be consulted for advice.

8. Key Partnerships:
The key partnerships are with the customers as well, as they provide data to improve the model.

9. Cost Structure:
There are currently no costs, as this is done volunatarily on behalf of the module "Programming Languages for Data Science".

## Frame Problem

We are investigating the characteristics of startups
Because we want to find out, if a startup will be successful
In order to decide to invest in it or not.

We want the model to classify the status of a startup
Our ideal outcome is "acquired", which means that the startup was successful
In order to reduce the uncertainty in startup investments.

## Identify Variables

For structured data problems, we need to identify potentially relevant variables.
The response variable is the categorical status (acquired or closed - if a startup is ‘acquired’ by some other organization, means the startup succeeded) and there are many explanatory variables, which include the category, location, funding rounds and amount.
The [data set](https://www.kaggle.com/datasets/manishkc06/startup-success-prediction?resource=download&select=startup+data.csv) is from kaggle and contains data about startups since the late 1990s in the United States of America.

## Define Metrics

Our success metrics are accuracy (percentage of startups that the model correctly predicts as successful or unsuccessful) and precision (indicates that the model is making fewer false positive predictions).
Our key results for the success metrics are an accuracy and precision of more than 90%.
Our project is deemed a failure, if the accuracy and precision are lower than 90%.

# Data

## Data Ingestion
### Prepare Environment

At first, R is set up.
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Then the necessary libraries for the entire notebook are activated and have been installed before.
```{r libraries}
library(tidyverse)
library(conflicted)
library(dplyr)
library(visdat)
library(rsample)
library(tidymodels)
library(skimr)
library(purrr)
library(GGally)
library(DBI)
library(maps)
library(mapproj)
```

### Import Data 

Currently csv upload, could chnage this to upload from a database.

```{r}
path <- "/Users/helena.schick/Documents/GitHub/r-sql-startup-success/startup data.csv"

df <- read_csv(path)
```

### Data Structure

Now, we're having a first look at the data, including the column names, data types and their content.
```{r}
glimpse(df)
```
The column `Unnamed: 0` seems to be some kind of id, but has no further value and will be deleted. Labels and avg_participants cannot be understood without further context. `Unnamed: 6` is a combination of the zip code and city and thus redundant and will be deleted. The redundancy also accounts for state_code.1 and object_id.
For categories and states there are each a categorical variable and several one hot encoded ones. All are kept at this point, because the categorical is better for visualizations and data understanding, but the one hot encoded ones are needed for the classification model.
The variables are described in more detail:
| Variable | Description | Data Type |
| ------ | --: | ----------|
| state_code |  Abbreviation of US state of startup's location | character |
| latitude | Geographic North-South location of startup's location | double |
| longitude | Geographic East-West location of startup's location | double |
| zip_code | Zip code of the city of startup's location | double |
| id | Unique identification of startup | string |
| city | Name of city of startup's location | string |
| name | Name of startup | string |
| founded_at | Founding date of startup (MM/DD/YYYY) | date |
| closed_at | Closing date of startup (MM/DD/YYYY) | date |
| first_funding_at | Date of first funding received of startup (MM/DD/YYYY) | date |
| last_funding_at | Date of last funding received of startup (MM/DD/YYYY) | date |
| age_first_milestone_year | Age of startup when achieving first milestone | double |
| age_last_milestone_year | Age of startup when achieving last milestone | double |
| relationships | Amount of relationships of startup with investors, mentors,... | double |
| funding_rounds | Amount of funding rounds | double |
| funding_total_usd | Total amount of funding received in USD | double |
| milestones | Amount of milestones achieved | double |
| is_CA | Startup's location is in CA (1 = true, 0 = false) | double |
| is_NY | Startup's location is in NY (1 = true, 0 = false) | double |
| is_MA | Startup's location is in MA (1 = true, 0 = false) | double |
| is_TX | Startup's location is in TX (1 = true, 0 = false) | double |
| is_otherstate | Startup's location is in another state than CA, NY, MA or TX (1 = true, 0 = false) | double |
| category_code | Business category of startup | string |
| is_software | Startup's category is software (1 = true, 0 = false) | double |
| is_web | Startup's category is web (1 = true, 0 = false) | double |
| is_mobile | Startup's category is mobile (1 = true, 0 = false) | double |
| is_enterprise | Startup's category is enterprise (1 = true, 0 = false) | double |
| is_advertising | Startup's category is advertising (1 = true, 0 = false) | double |
| is_gamesvideo | Startup's category is gamesvideo (1 = true, 0 = false) | double |
| is_biotech | Startup's category is biotech (1 = true, 0 = false) | double |
| is_ecommerce | Startup's category is ecommerce (1 = true, 0 = false) | double |
| is_consulting | Startup's category is consulting (1 = true, 0 = false) | double |
| is_othercategory | Startup's category is another category than the previously mentioned (1 = true, 0 = false) | double |
| has_VC | Startup is financed through Venture Capital (1 = true, 0 = false) | double |
| has_angel | Startup is financed through an angel investor (1 = true, 0 = false) | double |
| has_roundA | Startup has succeeded in first major funding round (1 = true, 0 = false) | double |
| has_roundB | Startup has succeeded in second major funding round (1 = true, 0 = false) | double |
| has_roundC | Startup has succeeded in third major funding round (1 = true, 0 = false) | double |
| has_roundD | Startup has succeeded in fourth major funding round (1 = true, 0 = false) | double |
| is_top500 | Startup is listed in the Top500 startups (1 = true, 0 = false) | double |
| status | Success variable of startup (acquired = successful, closed = unsuccessful) | string |

To understand the ratio of numerical to categorical variables and view the amount of null values in the dataframe, we visualize it.
```{r}
vis_dat(df)
```
The majority of variables are numeric. There are many null values in the columns Unnamed:6, which will be deleted anyways, as well as closed_at, because this only applies, if the startup has failed, and age_first_milestone and age_last_milestone. We are keeping these two variables to get some insights about those startups that have them available.

### Data Corrections

At first, all variables of the data type character are turned into factors. This has the advantages of memory efficiency, ordering, handling of missing values, visualization and improved functionality.
```{r}
# convert all character variables to factors 
df <- 
  df %>% 
  mutate(across(where(is.character), as.factor))
```

Secondly, the variables taht represent dates are turned into the data type date.
```{r}
# convert all date variables to actual dates 
df$founded_at <- 
  as.Date(df$founded_at, format = "%m/%d/%Y")
df$closed_at <- 
  as.Date(df$closed_at, format = "%m/%d/%Y")
df$first_funding_at <- 
  as.Date(df$first_funding_at, format = "%m/%d/%Y")
df$last_funding_at <- 
  as.Date(df$last_funding_at, format = "%m/%d/%Y")
```

In the end, unnecessary varibales are removed.
```{r}
# remove unnecessary variables
df <- 
  dplyr::select(df, -`Unnamed: 0`, -`Unnamed: 6`, -state_code.1, -object_id, -labels, -avg_participants)
```

### Variable List

For easier usage in data splitting, several variable lists are created.
```{r}
# define outcome variable as y_label
y_label <- 'status'

# select feature names
features <- 
  df %>%
  select(-all_of(y_label)) %>%
  names()

# create feature data for data splitting
X <- 
  df %>%
  select(all_of(features))

# list of numeric feature names
feat_num <- 
  X %>% 
  select(where(is.numeric)) %>% 
  names()

# list of categorical feature names
feat_cat <- 
  X %>% 
  select(!where(is.numeric)) %>% 
  names()

# create response for data splitting
y <- 
  df %>% 
  select(all_of(y_label))
```

## Data Splitting

### Train and Test Split

The dataframe is split into a training and test set. The training set will be used to train the model and the the test set is used to verify how well the model performs with newly added data.
```{r}
# Fix the random numbers by setting the seed 
# This enables the analysis to be reproducible 
set.seed(42)

# Put 3/4 of the data into the training set 
data_split <- initial_split(df, 
                           prop = 3/4, 
                           strata = y_label, 
                           breaks = 4)

# Create dataframes for the two sets:
train_data <- training(data_split) 
test_data <- testing(data_split)
```
From the training data another split is performed to create validation data. This is used to verify the model before adding new data to avoid overfitting of the model to the training data.  
```{r}
set.seed(42)

cv_folds <- 
  vfold_cv(train_data,
           v=5,
           strata = y_label,
           breaks = 4)
```


### Data Exploration Set

A copy of the training data is created for the data exploration to not alter the actual training data.
```{r}
df_train <- train_data 
```

## Analyze Data

Data analysis is done through R and SQL. 

### SQL Analysis

To use SQL in RStudio a connection is created and the following code cells start with {sql connection=...} instead of {r}.
```{r}
# Connection to database SQlite
con <- dbConnect(RSQLite::SQLite(), ":memory:")

# Write data "df_train" into database
dbWriteTable(con, "df_train", df_train)

# List tables
dbListTables(con)
```

At first, we want to get an overview of the training data and have a look at the first 5 rows.
```{sql connection=con}
SELECT *
FROM df_train
LIMIT 5;
```

Count all distinct observations with the status "acquired".
```{sql connection=con}
SELECT DISTINCT COUNT(*)
FROM df_train
WHERE status = 'acquired';
```
There are 447 startups in the training data that are successful.

Count all distinct observations with the status "closed".
```{sql connection=con}
SELECT DISTINCT COUNT(*)
FROM df_train
WHERE status = 'closed';
```
There are 244 startups in the training data that are not successful. Neither of these too small to worry about an uneven distribution for the model performance later on.

Which successful startups receive the most funding? Also providing more information about these companies.
```{sql connection=con}
SELECT name, category_code, state_code, funding_total_usd, age_first_funding_year, funding_rounds
FROM df_train
WHERE status = "acquired"
ORDER BY funding_total_usd DESC
LIMIT 5;
```
The startups that received the most funding are Clearwire, Pearl Therapeutics and Luminus Devices.

In which state and category is the most funding raised?
```{sql connection=con}
SELECT category_code, state_code, SUM(funding_total_usd)
FROM df_train
WHERE status = "acquired"
ORDER BY SUM(funding_total_usd) DESC;
```
Most funding is raised in the music business in California.

In which categories are most successful startups founded?
```{sql connection=con}
SELECT category_code, COUNT(status = "acquired") as count
FROM df_train
GROUP BY category_code
ORDER BY count DESC;
```
Most successful startups are in the software and web business. There are only very few successful startups in the categories sports, hospitality, health and automotive.

In which states are most successful startups founded?
```{sql connection=con}
SELECT state_code, COUNT(status = "acquired") as count
FROM df_train
GROUP BY category_code
ORDER BY count DESC;
```
Most successful startups are founded in Colorado and California.

How many successful vs. unsuccessful startups have Round A funding?
```{sql connection=con}
SELECT COUNT(*)
FROM df_train
WHERE has_roundA = 1 AND status = "acquired";
```
```{sql connection=con}
SELECT COUNT(*)
FROM df_train
WHERE has_roundA = 1 AND status = "closed";
```
```{sql connection=con}
SELECT COUNT(*)
FROM df_train
WHERE has_roundA = 1 AND status = "acquired" and age_first_funding_year < 2;
```
```{sql connection=con}
SELECT COUNT(*)
FROM df_train
WHERE has_roundA = 1 AND status = "closed" and age_first_funding_year < 2;
```
256 startups that have a Round A funding are successful, 97 are not. Looking at young startups, who received their first funding before turning two years old, 202 received Round A funding, but 66 failed. This shows that most startups are younger than two years when receiving their Round A funding, but it's hard to predict whether they will succeed.

Now looking at the same comparison for Round B funding.
```{sql connection=con}
SELECT COUNT(*)
FROM df_train
WHERE has_roundB = 1 AND status = "acquired";
```
```{sql connection=con}
SELECT COUNT(*)
FROM df_train
WHERE has_roundB = 1 AND status = "closed";
```
213 startups who receive Round B funding succeed and 62 fail. As these numbers are not tremendously lower than the round A funding, this indicates that many startups who received a Round A funding, also get Round B funding. The ratio of succeeding startups has risen a lot from Round A to Round B, so the investment is already more certain.

Having a closer look at the next funding round, Round C.
```{sql connection=con}
SELECT COUNT(*)
FROM df_train
WHERE has_roundc = 1 AND status = "acquired";
```
```{sql connection=con}
SELECT COUNT(*)
FROM df_train
WHERE has_roundC = 1 AND status = "closed";
```
These number got quite lower, as only 135 of startups in Round C funding succeed and 33 fail.

And having a closer look at the last funding round, Round D.
```{sql connection=con}
SELECT COUNT(*)
FROM df_train
WHERE has_roundD = 1 AND status = "acquired";
```
```{sql connection=con}
SELECT COUNT(*)
FROM df_train
WHERE has_roundD = 1 AND status = "closed";
```
Comparably few startups make it to Round D, only 54 succeed and 9 fail.

Other investment options are also considered. First having a look at the success of startups that have venture capital.
```{sql connection=con}
SELECT COUNT(*)
FROM df_train
WHERE has_vc = 1 AND status = "acquired";
```
```{sql connection=con}
SELECT COUNT(*)
FROM df_train
WHERE has_vc = 1 AND status = "closed";
```
139 startups that have venture capital succeed and 85 fail. This ratio is worse than in funding rounds.

How about business angel investments?
```{sql connection=con}
SELECT COUNT(*)
FROM df_train
WHERE has_angel = 1 AND status = "acquired";
```
```{sql connection=con}
SELECT COUNT(*)
FROM df_train
WHERE has_angel = 1 AND status = "closed";
```
The ratio of failing to succeeding startups, who have a business angel investment, is the worst in comparison to other invetsment options, 77 fail and 93 succeed.

How many funding rounds to successful vs. failed startups have?
```{sql connection=con}
SELECT AVG(funding_rounds)
FROM df_train
WHERE status = "acquired";
```
```{sql connection=con}
SELECT AVG(funding_rounds)
FROM df_train
WHERE status = "closed";
```
Successful startups have more funding_rounds than failed ones, which makes total sense, because most startups go through all funding rounds before being acquired.

How does the duration between achieving major milestones differ between successful vs. failed startups?
```{sql connection=con}
SELECT AVG(age_last_milestone_year - age_first_milestone_year) as avg_milestoneduration
FROM df_train
WHERE status = "acquired";
```

```{sql connection=con}
SELECT AVG(age_last_milestone_year - age_first_milestone_year) as avg_milestoneduration
FROM df_train
WHERE status = "closed";
```
The average milestone duration is almost double, with roughly 2 years between milestones, for successful startups versus failed ones. 

### Categorical Features

For every categorical feature the count of each category is created and the 10 most frequent ones are presented.
```{r}
n <- 10  # Number of most frequent levels to show
for (i in feat_cat){
  
  counts <- df_train %>% count(!!sym(i)) %>% arrange(desc(n)) %>% head(n)
  
  p <- ggplot(counts, aes_string(x=i, y="n")) +
    geom_bar(stat="identity")
  
  plot(p)
}
```
The most common states are California by far, followed by New York and Massachusetts. This is also represented in the zip codes, as the most frequent ones start with 9 which indicates CA. And the most common cities are San Francisco and New York. It will be interesting to look at visualization of this geographcal data later.
There are one duplicate id and name that have to be deleted.
The founding dates are quite evenly spread out between 1999 and 2009. The closing dates of failed stratups are high in 2012 and 2013 and the first one closed already in 2009.
The first funding rounds took place in 2005 to 2008 and the last funding rounds in 2006 until 2012.MostLast funding rounds were in 2008.
The most common categories of startups are web and software.


### Numerical Features

After analyzing single categorical features, the same is done for numerical features. At first the scales of each feature are analyzed, including the quartiles, mean and median. Using skim, this also includes a rough histogram.
```{r}
skim(df_train)
```
Describe skim results.

```{r}
# Removing one hot encoded features
df_numeric <- df_train %>% 
  select(all_of(feat_num))
df_numeric <- df_numeric %>%
  select(-is_CA, -is_NY, -is_MA, -is_TX, -is_otherstate, -is_software, -is_web, -is_mobile, -is_enterprise, -is_advertising, -is_gamesvideo, -is_ecommerce, -	is_biotech, -is_consulting, -is_enterprise, -is_othercategory, -has_VC, -has_angel, -has_roundA, -has_roundB, -has_roundC, -has_roundD, -is_top500)

# Create histograms using ggplot2
hist_list <- lapply(names(df_numeric), function(col) {
  ggplot(df_numeric, aes(x = .data[[col]])) +
  geom_histogram(bins = 10) +
  ggtitle(paste0(col))
})

# Arrange histograms in a grid
library(patchwork)
hist_list[[1]] + hist_list[[2]] + hist_list[[3]] + plot_layout(ncol = 3)
hist_list[[4]] + hist_list[[5]] + hist_list[[6]] + plot_layout(ncol = 3)
hist_list[[7]] + hist_list[[8]] + hist_list[[9]] + plot_layout(ncol = 3)
```
Describe more learnings about numeric features from histograms.


### Relationships

correlation
```{r}
df_numeric %>% 
  vis_cor(cor_method = "spearman", na_action = "pairwise.complete.obs")
```
Describe correlations

If not linear chekcing other relations
```{r}
df_numeric %>% 
  ggscatmat()
```
Desribe rerelations

Comparison Relationship by status
```{r}
df_train %>%
  ggplot(aes(x = status, y = relationships)) +
  geom_boxplot() +
  labs(title = "Relationships, if successful or not") +
  theme_bw(base_size = 12) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5))
```
Description of results

Further Viz
```{r}
df_train %>%
  ggplot(aes(x = relationships, y = funding_rounds, group = status, color = status)) +
  geom_point() +
  theme_classic(base_size = 12) +
  ggtitle("??") +
  theme(legend.title = element_blank())
```
Description

Map of locations acquired vs. closed
```{r}
world_map <- map_data("world")

ggplot(df_train, aes(x = longitude, y = latitude, color = status)) +
  geom_point() +
  geom_path(data = world_map, aes(x = long, y = lat, group = group), color = "gray50") +
  labs(x = "Longitude", y = "Latitude", title = "Startup Locations")
```
Heatmap acquired nach age_first_funding_year
```{r}
world_map <- map_data("world")

df_ac <- df_train %>% 
  dplyr::filter(status == "acquired")

ggplot(df_ac, aes(x = longitude, y = latitude, color = age_first_funding_year)) +
  geom_point() +
  geom_path(data = world_map, aes(x = long, y = lat, group = group), color = "gray50") +
  labs(x = "Longitude", y = "Latitude", title = "Startup Locations")
```

## Define Schema
Usually it is a good idea to define some sort of schema that describes the expected properties of the data.
check_class()
Check Variable Class
```{r}
recipe(train_data, status ~ .) %>%
  check_class(everything()) %>%
  prep(train_data, strings_as_factors = FALSE) %>%
  bake(test_data)
```

check_cols()
Check if all Columns are Present
```{r}
#needs to be adjusted to this df
#biomass_rec <- recipe(HHV ~ ., data = biomass) %>%
#  step_rm(sample, dataset) %>%
#  check_cols(contains("gen")) %>%
#  step_center(all_numeric_predictors())
#if (FALSE) {
#bake(biomass_rec, biomass[, c("carbon", "HHV")])
#}
```

check_missing()
Check for Missing Values
```{r}
is.na(df_train) %>% colSums()
```

check_new_values()
Check for New Values
```{r}
#recipe(df_train) %>%
#  check_new_values(status) %>%
#  prep() %>%
#  bake(new_data = df_train)
```
check_range()
Check Range Consistency
```{r}
#slack_df <- df_train(relationships = 0:100)
#slack_new_data <- df_train(relationships = -10:110)

# this will fail the check both ends
#if (FALSE) {
#recipe(slack_df) %>%
#  check_range(relationships) %>%
#  prep() %>%
#  bake(slack_new_data)
#}
```


## Anomaly Detection

delete duplicate id and name

### Missing Values

### Outlier Detection

## Feature Engineering
Feature engineering is the process of using domain knowledge to extract meaningful features (attributes) from raw data. The goal of this process is to create new features which improve the predictions from our model.

### Pipelines

### Feature Transformation

#### Fix Missing Values

#### Fix Outliers

#### Feature Scaling
Standardization

#### Encoding

### Feature Extraction

### Custom Feature Operations

### Final Data Pipeline

# Model
## Select Algorithm
Supervised Learning
Classification
kNN
Naive Bayes
SVM 
Decision Tree
Logistic Regression

## Model Training & Tuning
### Feature Selection

### Training & Hyperparameter Tuning

### Evaluation
Evaluate best model on wrongest predictions

## Evaluate Model
Evaluate the final model on test data

# Deployment
RStudio's Model Management: https://solutions.posit.co/gallery/model-management/vetiver/

WebAPI with Plumber: https://www.rplumber.io/articles/introduction.html

## Validate Model

## Deploy Model

## Serve Model

## Monitor Model