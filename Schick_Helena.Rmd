---
title: "Startup Success Prediction"
subtitle: "Programming Languages - Project by Helena Schick"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_depth: 3
    number_sections: true
editor_options: 
  markdown: 
    wrap: 72
---

This project to predict the success of startups is conducted according to the Data Science Lifecycle.

<img src ="https://kirenz.github.io/data-science-r/_images/lifecycle.png"/>

# Plan

## Identify Use Case

The objective of this project is to predict the success of startups. A
startup is a new company founded by an entrepreneur with the intent to
grow beyond the solo founder according to
[wikipedia](https://en.wikipedia.org/wiki/Startup_company). Startups
play major role in the economy, as they foster innovation and create
employment as they grow. Yet, they face high uncertainty and need to
find investors to continue their ideas and expand their potential. This
project focuses on the investors to find companies with great potential
to invest in and thus being one step ahead of the competition. A
business model canvas is used to describe the use case in more detail.

1.  Customer Segments: The costumers of this use case are investors that
    want to investigate the startup landscape quickly and want to make
    the best possible investment. The investors' biggest pain is the
    uncertainty of the investment in a startup. Even with a
    well-developed business plan and promising market research, there is
    no guarantee that the startup will be successful. Furthermore,
    startups are risky ventures, as they often have limited
    operating history and may face significant challenges in scaling up
    their operations, acquiring customers, and generating revenue.
    Moreover, investing in a startup requires a significant time
    commitment. Investors need to research potential opportunities,
    evaluate business plans, and actively monitor their investments.
    Their gains include high returns in the case that the startup will
    be successful. Especially early-stage investors have the opportunity
    to purchase equity at a low valuation, which can result in a
    significant return on investment if the company is acquired or goes
    public. In addition, investing in startups can help diversify an
    investor's portfolio. Startups can provide exposure to new market
    segments that may be difficult to access through traditional
    investments. Furthermore, some investors may be motivated by the
    social impact that startups can have. By investing in innovative
    companies that are addressing social or environmental challenges,
    investors can support positive change while also generating
    financial returns.

2.  Value Proposition: This projects addresses all of these pain points
    by providing detailed analysis of many variables former startups to
    transfer these learnings to new investments. It highlights the most
    important features of a startup to become successful and predicts
    whether a startup which is currently operating turns into a success
    or a failure. The success of a company is defined as the event that
    gives the company's founders a large sum of money through the
    process of Merger and Acquisition or an Initial Public
    Offering. A company would be considered as failed if it had to be
    shut down. This minimizes the uncertainty and risk of the
    investment. As this is provided in one platform, the time commitment
    and intense research is reduced tremendously. The data can always be
    extended through more startup data provided by the customers. This
    has the advantage that possible changes in the successful features
    are encountered and the use case always provides the latest and most
    valuable information.

3.  Channels: Specific investors will be contacted directly to learn
    about this startup success prediction and how to purchase it. In
    addition, the use case will be advertised on business platforms and
    online newspapers.

4.  Customer Relationships: A close customer collaboration will be
    created, that the investors have the information about startup
    success prediction and also share information about their past
    investments that the data is always improving and up to date.

5.  Revenue Streams: The startup success prediction can be purchased as
    a yearly abonnement by investors. They can get a reduction of the
    fee in exchange for startup investment data that can be incorporated
    in the analysis and prediction.

6.  Key Activities: The most important activity is always having a well
    optimized classification model for the startup success prediction.
    Another key activity is the data analysis that is used as a base for
    the prediction model. The third activity is to create a good user
    experience to visualize the key learnings and recommendations.

7.  Key Resources: The project is conducted by Helena Schick. Jan Kirenz
    can be consulted for advice.

8.  Key Partnerships: The key partnerships are with the customers as
    well, as they provide data to improve the model.

9.  Cost Structure: There are currently no costs, as this is done
    voluntarily on behalf of the module "Programming Languages for Data
    Science".

## Frame Problem

We are investigating the characteristics of startups, because we want to
find out, if a startup will be successful in order to decide to invest
in it or not.

We want the model to classify the status of a startup. Our ideal outcome
is "acquired", which means that the startup was successful in order to
reduce the uncertainty in startup investments.

## Identify Variables

The response variable is the categorical status (acquired or
closed - if a startup is 'acquired' by some other organization, means
the startup succeeded) and there are many explanatory variables, which
include the category, location, funding rounds and amount. The [data
set](https://www.kaggle.com/datasets/manishkc06/startup-success-prediction?resource=download&select=startup+data.csv)
is from kaggle and contains data about several hundred startups.

## Define Metrics

Our success metrics are accuracy (percentage of startups that the model
correctly predicts as successful or unsuccessful) and precision
(indicates that the model is making fewer false positive predictions).
Our key results for the success metrics are an accuracy and precision of
more than 80%. Our project is deemed a failure, if the accuracy and
precision are lower than 80%.

# Data

## Data Ingestion

### Prepare Environment

At first, R is set up.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Then the necessary libraries for the entire notebook are activated and
have been installed before.

```{r libraries}
library(tidyverse)
library(conflicted)
library(dplyr)
library(visdat)
library(rsample)
library(tidymodels)
library(skimr)
library(purrr)
library(GGally)
library(DBI)
library(maps)
library(mapproj)
library(caret)
library(xgboost)
library(kknn)
library(vip)
library(tidy.outliers)
```

### Import Data

First, the .csv file on startups is uploaded.

```{r}
path <- "/Users/helena.schick/Documents/GitHub/r-sql-startup-success/startup data.csv"

df <- read_csv(path)
```

### Data Structure

Now, we're having a first look at the data, including the column names,
data types and their content.

```{r}
glimpse(df)
```

The column `Unnamed: 0` seems to be some kind of id, but has no further
value and will be deleted. Labels and avg_participants cannot be
understood without further context. `Unnamed: 6` is a combination of the
zip code and city and thus redundant and will be deleted. The redundancy
also accounts for state_code.1 and object_id. For categories and states
there are each a categorical variable and several one hot encoded ones.
All are kept at this point, because the categorical is better for
visualizations and data understanding, but the one hot encoded ones are
needed for the classification model. The variables are described in more
detail: 

| Variable | Description | Data Type |
| :----- | :----- | :----- |
| state_code | Abbreviation of US state of startup's location | character |
| latitude | Geographic North-South location of startup's location | double |
| longitude | Geographic East-West location of startup's location | double |
| zip_code | Zip code of the city of startup's location | double |
| id | Unique identification of startup | string |
| city | Name of city of startup's location | string |
| name | Name of startup | string |
| founded_at | Founding date of startup (MM/DD/YYYY) | date |
| closed_at | Closing date of startup (MM/DD/YYYY) | date |
| first_funding_at | Date of first funding received of startup (MM/DD/YYYY) | date |
| last_funding_at | Date of last funding received of startup (MM/DD/YYYY) | date |
| age_first_milestone_year | Age of startup when achieving first milestone | double |
| age_last_milestone_year | Age of startup when achieving last milestone | double |
| relationships | Amount of relationships of startup with investors, mentors,... | double |
| funding_rounds | Amount of funding rounds | double |
| funding_total_usd | Total amount of funding received in USD | double |
| milestones | Amount of milestones achieved | double |
| is_CA | Startup's location is in CA (1 = true, 0 = false) | double |
| is_NY | Startup's location is in NY (1 = true, 0 = false) | double |
| is_MA | Startup's location ismin MA (1 = true, 0 = false) | double |
| is_TX | Startup's location is in TX (1 = true, 0 = false) | double |
| is_otherstate | Startup's location is in another state than CA, NY, MA or TX (1 = true, 0 = false) | double |
| category_code | Business category of startup | string |
| is_software | Startup's category is software (1 = true, 0 = false) | double |
| is_web | Startup's category is web (1 = true, 0 = false) | double |
| is_mobile | Startup's category is mobile (1 = true, 0 = false) | double |
| is_enterprise | Startup's category is enterprise (1 = true, 0 = false) | double |
| is_advertising | Startup's category is advertising (1 = true, 0 = false) | double |
| is_gamesvideo | Startup's category is gamesvideo (1 = true, 0 = false) | double |
| is_biotech | Startup's category is biotech (1 = true, 0 = false) | double |
| is_ecommerce | Startup's category is ecommerce (1 = true, 0 = false) | double |
| is_consulting | Startup's category is consulting (1 = true, 0 = false) | double |
| is_othercategory | Startup's category is another category than the previously mentioned (1 = true, 0 = false) | double |
| has_VC | Startup is financed through Venture Capital (1 = true, 0 = false) | double |
| has_angel | Startup is financed through an angel investor (1 = true, 0 = false) | double |
| has_roundA | Startup has succeeded in first major funding round (1 = true, 0 = false) | double |
| has_roundB | Startup has succeeded in second major funding round (1 = true, 0 = false) | double |
| has_roundC | Startup has succeeded in third major funding round (1 = true, 0 = false) | double |
| has_roundD | Startup has succeeded in fourth major funding round (1 = true, 0 = false) | double |
| is_top500 | Startup is listed in the Top500 startups (1 = true, 0 = false) | double |
| status | Success variable of startup (acquired = successful, closed = unsuccessful) | string |

To understand the ratio of numerical to categorical variables and view
the amount of null values in the data frame, we visualize it.

```{r}
vis_dat(df)
```

The majority of variables are numeric. There are many null values in the
columns Unnamed:6, which will be deleted anyways, as well as closed_at,
because this only applies, if the startup has failed, and
age_first_milestone and age_last_milestone. We are keeping these two
variables to get some insights about those startups that have them
available.

### Data Corrections

At first, all variables of the data type character are turned into
factors. This has the advantages of memory efficiency, ordering,
handling of missing values, visualization and improved functionality.

```{r}
# convert all character variables to factors 
df <- 
  df %>% 
  mutate(across(where(is.character), as.factor))
```

Secondly, the variables that represent dates are turned into the data
type date.

```{r}
# convert all date variables to actual dates 
df$founded_at <- 
  as.Date(df$founded_at, format = "%m/%d/%Y")
df$closed_at <- 
  as.Date(df$closed_at, format = "%m/%d/%Y")
df$first_funding_at <- 
  as.Date(df$first_funding_at, format = "%m/%d/%Y")
df$last_funding_at <- 
  as.Date(df$last_funding_at, format = "%m/%d/%Y")
```

In the end, unnecessary variables are removed.

```{r}
# remove unnecessary variables
df <- 
  dplyr::select(df, -`Unnamed: 0`, -`Unnamed: 6`, -state_code.1, -object_id, -labels, -avg_participants)
```

```{r}
write.csv(df, "/Users/helena.schick/Documents/GitHub/r-sql-startup-success/df.csv", row.names=FALSE)
```

We create a separate data frame that excludes all variables that have insights on the funding rounds and progress. With this information another classification model is created for early success prediction of very new startups that are in or before their first funding round.

```{r}
df_new <- 
  dplyr::select(df, -closed_at, -last_funding_at, -age_last_milestone_year, -funding_rounds, -funding_total_usd, -milestones, -has_VC, -has_angel, -has_roundA, -has_roundB, -has_roundC, -has_roundD, -is_top500)
```

### Variable List

For easier usage in data splitting, several variable lists are created.

```{r}
# define outcome variable as y_label
y_label <- 'status'

# select feature names
features <- 
  df %>%
  select(-all_of(y_label)) %>%
  names()

# create feature data for data splitting
X <- 
  df %>%
  select(all_of(features))

# list of numeric feature names
feat_num <- 
  X %>% 
  select(where(is.numeric)) %>% 
  names()

# list of categorical feature names
feat_cat <- 
  X %>% 
  select(!where(is.numeric)) %>% 
  names()

# create response for data splitting
y <- 
  df %>% 
  select(all_of(y_label))
```

## Data Splitting

### Train and Test Split

The data frame is split into a training and test set. The training set
will be used to train the model and the the test set is used to verify
how well the model performs with newly added data.

```{r}
# Fix the random numbers by setting the seed 
# This enables the analysis to be reproducible 
set.seed(42)

# Put 3/4 of the data into the training set 
data_split <- initial_split(df, 
                           prop = 3/4, 
                           strata = status, 
                           breaks = 4)

# Create dataframes for the two sets:
train_data <- training(data_split) 
test_data <- testing(data_split)
```

```{r}
# Same for df_new
set.seed(42)

# Put 3/4 of the data into the training set 
data_split_new <- initial_split(df_new, 
                           prop = 3/4, 
                           strata = status, 
                           breaks = 4)

# Create dataframes for the two sets:
train_data_new <- training(data_split_new) 
test_data_new <- testing(data_split_new)
```

### Data Exploration Set

A copy of the training data is created for the data exploration to not
alter the actual training data. This step and the following data analysis is done on the big data frame only.

```{r}
df_train <- train_data 
```

## Analyze Data

Data analysis is done through R and SQL. It gives insights in the
distribution of each feature, as well as patterns and correlations. At
first the scales of each feature are analyzed, including the quartiles,
mean and median. Using skim, this also includes a rough histogram.

```{r}
skim(df_train)
```

**Dates:**

The founded_at ranges between 1984 and 2013. Its median is in 2006,
that's why founding dates in the 20th might be outliers. Closed_at is
very right skewed with the median in 2012 and maximum in 2013, but the
earliest closing in 2001. The first_funding_at seems be be quite evenly
distributed, the same applies to last_founding_at.

**Categorical features:**

There are startups in 31 states, but CA is most common. Dividing the
location up into zip codes, there are 319 different ones in 184 cities.
The name and id are almost unique, having one duplicate, which needs to
be removed. The startup are in 35 different categories and 2 statuses.

**Numerical features:**

The latitude ranges between 25.75° and 59.33°, where most startups are
located at around 37°, because the second and quartile as well as the
mean are around that value. The longitude ranges from -122.72° to 18.05°
and is left skewed, because the first and second quartile are at around
-122°. The bigger values might include some outliers. The latitude and
longitude will be visualized on a map, because it is hard to imagine
their distribution by looking at values. The age_first_funding_year has
a mean of 2.17 years and a median of 1.34 years, which indicated a right
skewed distribution. The maximum of over 20 years seems to be an
outlier. A similar distribution applies to age_last_funding_year, with a
maximum of over 20 years. Both have negative minima, which doesn't
make sense as the funding would have taken place before the founding.
These outliers need to be investigated. Age_first_milestone_year and
age_last_milestone_year have a similar ditribution as the first and last
funding ages. They also have negative minima, which doesn't make sense
as the milestone would have taken place before the founding. These
outliers need to be investigated. Most startups have between 5 and 10
relationships, the minimum is 0 and the maximum 57. All startups in this
dataframe have been through at least 1 funding round and in average
through 2. The maximum of 10 funding rounds seems to be an outlier, as
usually there are only the 4 funding rounds A to D. This needs to be
investigated. Funding_total_usd is left skewed, as the mean is almost 3
times higher than the mean. Outliers on the upper end need to be
investigated. Most startups have between 0 and 3 milestones, the maximum
is 8. 53% of the startups are located in CA, 11% in NY, 9% in MA and 4%
in Texas. Also 17% of the startups are in software and 16% in web. It
doesn't make sense to further consider these one hot encoded variables
in the numeric feature analysis. This also applies to the funding
variables, but 32% of the startups have Venture Capital and 25% a
business angel. 51% succeed in Round A funding, only 40% in Round B, 24%
in Round C and only 9% in Round D. 81% made it to the Top500 though.

### SQL Analysis

To use SQL in RStudio a connection is created and the following code
cells start with {sql connection=...} instead of {r}.

```{r}
# Connection to database SQlite
con <- dbConnect(RSQLite::SQLite(), ":memory:")

# Write data "df_train" into database
dbWriteTable(con, "df_train", df_train)

# List tables
dbListTables(con)
```

At first, we want to get an overview of the training data and have a
look at the first 5 rows.

```{sql connection=con}
SELECT *
FROM df_train
LIMIT 5;
```

Count all distinct observations with the status "acquired".

```{sql connection=con}
SELECT DISTINCT COUNT(*)
FROM df_train
WHERE status = 'acquired';
```

There are 447 startups in the training data that are successful.

Count all distinct observations with the status "closed".

```{sql connection=con}
SELECT DISTINCT COUNT(*)
FROM df_train
WHERE status = 'closed';
```

There are 244 startups in the training data that are not successful.
Neither of these too small to worry about an uneven distribution for the
model performance later on.

Which successful startups receive the most funding? Also providing more
information about these companies.

```{sql connection=con}
SELECT name, category_code, state_code, funding_total_usd, age_first_funding_year, funding_rounds
FROM df_train
WHERE status = "acquired"
ORDER BY funding_total_usd DESC
LIMIT 5;
```

The startups that received the most funding are Clearwire, Pearl
Therapeutics and Luminus Devices.

In which state and category is the most funding raised?

```{sql connection=con}
SELECT category_code, state_code, SUM(funding_total_usd)
FROM df_train
WHERE status = "acquired"
ORDER BY SUM(funding_total_usd) DESC;
```

Most funding is raised in the music business in California.

In which categories are most successful startups founded?

```{sql connection=con}
SELECT category_code, COUNT(status = "acquired") as count
FROM df_train
GROUP BY category_code
ORDER BY count DESC;
```

Most successful startups are in the software and web business. There are
only very few successful startups in the categories sports, hospitality,
health and automotive.

In which states are most successful startups founded?

```{sql connection=con}
SELECT state_code, COUNT(status = "acquired") as count
FROM df_train
GROUP BY state_code
ORDER BY count DESC;
```

Most successful startups are founded in California.

How many successful vs. unsuccessful startups have Round A funding?

```{sql connection=con}
SELECT COUNT(*)
FROM df_train
WHERE has_roundA = 1 AND status = "acquired";
```

```{sql connection=con}
SELECT COUNT(*)
FROM df_train
WHERE has_roundA = 1 AND status = "closed";
```

```{sql connection=con}
SELECT COUNT(*)
FROM df_train
WHERE has_roundA = 1 AND status = "acquired" and age_first_funding_year < 2;
```

```{sql connection=con}
SELECT COUNT(*)
FROM df_train
WHERE has_roundA = 1 AND status = "closed" and age_first_funding_year < 2;
```

256 startups that have a Round A funding are successful, 97 are not.
Looking at young startups, who received their first funding before
turning two years old, 202 received Round A funding, but 66 failed. This
shows that most startups are younger than two years when receiving their
Round A funding, but it's hard to predict whether they will succeed.

Now looking at the same comparison for Round B funding.

```{sql connection=con}
SELECT COUNT(*)
FROM df_train
WHERE has_roundB = 1 AND status = "acquired";
```

```{sql connection=con}
SELECT COUNT(*)
FROM df_train
WHERE has_roundB = 1 AND status = "closed";
```

213 startups who receive Round B funding succeed and 62 fail. As these
numbers are not tremendously lower than the round A funding, this
indicates that many startups who received a Round A funding, also get
Round B funding. The ratio of succeeding startups has risen a lot from
Round A to Round B, so the investment is already more certain.

Having a closer look at the next funding round, Round C.

```{sql connection=con}
SELECT COUNT(*)
FROM df_train
WHERE has_roundc = 1 AND status = "acquired";
```

```{sql connection=con}
SELECT COUNT(*)
FROM df_train
WHERE has_roundC = 1 AND status = "closed";
```

These number got quite lower, as only 135 of startups in Round C funding
succeed and 33 fail.

And having a closer look at the last funding round, Round D.

```{sql connection=con}
SELECT COUNT(*)
FROM df_train
WHERE has_roundD = 1 AND status = "acquired";
```

```{sql connection=con}
SELECT COUNT(*)
FROM df_train
WHERE has_roundD = 1 AND status = "closed";
```

Comparably few startups make it to Round D, only 54 succeed and 9 fail.

Other investment options are also considered. First having a look at the
success of startups that have venture capital.

```{sql connection=con}
SELECT COUNT(*)
FROM df_train
WHERE has_vc = 1 AND status = "acquired";
```

```{sql connection=con}
SELECT COUNT(*)
FROM df_train
WHERE has_vc = 1 AND status = "closed";
```

139 startups that have venture capital succeed and 85 fail. This ratio
is worse than in funding rounds.

How about business angel investments?

```{sql connection=con}
SELECT COUNT(*)
FROM df_train
WHERE has_angel = 1 AND status = "acquired";
```

```{sql connection=con}
SELECT COUNT(*)
FROM df_train
WHERE has_angel = 1 AND status = "closed";
```

The ratio of failing to succeeding startups, who have a business angel
investment, is the worst in comparison to other invetsment options, 77
fail and 93 succeed.

How many funding rounds to successful vs. failed startups have?

```{sql connection=con}
SELECT AVG(funding_rounds)
FROM df_train
WHERE status = "acquired";
```

```{sql connection=con}
SELECT AVG(funding_rounds)
FROM df_train
WHERE status = "closed";
```

Successful startups have more funding_rounds than failed ones, which
makes total sense, because most startups go through all funding rounds
before being acquired.

How does the duration between achieving major milestones differ between
successful vs. failed startups?

```{sql connection=con}
SELECT AVG(age_last_milestone_year - age_first_milestone_year) as avg_milestoneduration
FROM df_train
WHERE status = "acquired";
```

```{sql connection=con}
SELECT AVG(age_last_milestone_year - age_first_milestone_year) as avg_milestoneduration
FROM df_train
WHERE status = "closed";
```

The average milestone duration is almost double, with roughly 2 years
between milestones, for successful startups versus failed ones.

### Categorical Features

For every categorical feature the count of each category is created and
the 10 most frequent ones are presented.

```{r}
n <- 10  # Number of most frequent levels to show
for (i in feat_cat){
  
  counts <- df_train %>% count(!!sym(i)) %>% arrange(desc(n)) %>% head(n)
  
  p <- ggplot(counts, aes_string(x=i, y="n")) +
    geom_bar(stat="identity")
  
  plot(p)
}
```

The most common states are California by far, followed by New York and
Massachusetts. This is also represented in the zip codes, as the most
frequent ones start with 9 which indicates CA. And the most common
cities are San Francisco and New York. It will be interesting to look at
visualization of this geographical data later. There are one duplicate id
and name that have to be deleted. The founding dates are quite evenly
spread out between 1999 and 2009. The closing dates of failed startups
are high in 2012 and 2013 and the first one closed already in 2009. The
first funding rounds took place in 2005 to 2008 and the last funding
rounds in 2006 until 2012. Most last funding rounds were in 2008. The most
common categories of startups are web and software.

### Numerical Features

After analyzing single categorical features, the same is done for
numerical features.

```{r}
# Removing one hot encoded features
df_numeric <- df_train %>% 
  select(all_of(feat_num))
df_numeric <- df_numeric %>%
  select(-is_CA, -is_NY, -is_MA, -is_TX, -is_otherstate, -is_software, -is_web, -is_mobile, -is_enterprise, -is_advertising, -is_gamesvideo, -is_ecommerce, -	is_biotech, -is_consulting, -is_enterprise, -is_othercategory, -has_VC, -has_angel, -has_roundA, -has_roundB, -has_roundC, -has_roundD, -is_top500)

# Create histograms using ggplot2
hist_list <- lapply(names(df_numeric), function(col) {
  ggplot(df_numeric, aes(x = .data[[col]])) +
  geom_histogram(bins = 10) +
  ggtitle(paste0(col))
})

# Arrange histograms in a grid
library(patchwork)
hist_list[[1]] + hist_list[[2]] + hist_list[[3]] + plot_layout(ncol = 3)
hist_list[[4]] + hist_list[[5]] + hist_list[[6]] + plot_layout(ncol = 3)
hist_list[[7]] + hist_list[[8]] + hist_list[[9]] + plot_layout(ncol = 3)
```

The histogram for longitude verifies that the maximum value is an
outlier.There are two local maxima, one around -120° and another one at
around -80°, these are the West and East Coast of the United States. The
latitude of most startups is between 35° an 40°, the North and South of
the United States. For age_first_funding_year, age_last_funding_year,
age_first_milestone_year and age_last_milestone_year, the assumption of
outlier below 0 and higher than 10 is confirmed as well. The
distributions of relationships and funding_rounds look as I imagined
them. The maximum of funding_total_usd is an even more significant
outlier than expected, as the histogram only has one visible bin on the
very left. This needs to be removed.

### Relationships

The fist relationship analyzed is between the numerical features.
Correlation is a positive or negative linear relationship between two
variables. +1 and -1 indicate the highest possible linear correlation
and 0 indicated no correlation. As our label is categorical, it is not
possible to identify correlations to it with this method. But if the
correlations between features in the model is too high, this might lead
to overfitting. This method helps to identify those and evaluate which
to include in the model in the feature engineering chapter.

```{r}
df_numeric %>% 
  vis_cor(cor_method = "spearman", na_action = "pairwise.complete.obs")
```

There is a strong positive correlation (dark brown) between
age_first_funding_year and age_last_funding_year as well as
age_first_milestone_year and age_last_milestone_year. Another strong
positive correlation exists between funding_total_usd and
age_last_funding_year. Also milestones and relationships are positively
correlated. There are no strong negative correlation, the strongest is
between milestones and age_first_funding_year.

To make these results more precise, now the numbers of correlation are
displayed. In addition we have a look at the graphs of the relationships
to identify any non-linear relationships as well.

```{r}
df_numeric %>% 
  ggscatmat() +
  theme(axis.text.x = element_text(angle = 90, vjust = -1))
```

There are no non-linear relationships identified. The strong positive
correlation between the funding and milestone age variables is visible
in the scatter plots and verified through correlations \> 0.5.

At this, point further correlations between categorical and numerical
features area visualized. Do successful startups have more or less
relationships?

```{r}
df_train %>%
  ggplot(aes(x = status, y = relationships, color = status)) +
  geom_boxplot() +
  labs(title = "Relationships, if successful or not") +
  theme_bw(base_size = 12) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5))
```

Successful startups have more relationships than failes ones. Only 25%
of the successful startups have less than 7 relations, whereas 75% of
failed startups do have that amount.

Expanding on this. How is the amount of funding_rounds related to the
status and relationships?

```{r}
df_train %>%
  ggplot(aes(x = relationships, y = funding_rounds, group = status, color = status)) +
  geom_point() +
  theme_classic(base_size = 12) +
  ggtitle("Correlation of status, relationships and funding_rounds") +
  theme(legend.title = element_blank())
```

Acquired startups have more relationships and go through more funding
rounds.

As discussed earlier, it is easier to visualize longitude and
latitude than talking about their values. On the map the locations of
startups are visualized and by color their are distinguished by status.

```{r}
world_map <- map_data("world")

ggplot(df_train, aes(x = longitude, y = latitude, color = status)) +
  geom_point() +
  geom_path(data = world_map, aes(x = long, y = lat, group = group), color = "gray50") +
  labs(x = "Longitude", y = "Latitude", title = "Startup Locations")
```

There are a few startups located in Europe, but most are located in the
US. Many acquired startups are rather on the East of the US.

Another map visualization.

```{r}
library(ggmap)

qmplot(x = longitude, 
       y = latitude, 
       data = df_train, 
       geom = "point", 
       color = status,
       alpha = 0.4) +
  scale_alpha(guide = 'none') # don't show legend for alpha
```

Do successful startups have their first funding round earlier or later?

```{r}
df_train %>%
  ggplot(aes(x = status, y = age_first_funding_year, color = status)) +
  geom_boxplot() +
  labs(title = "Age first funding, if successful or not") +
  theme_bw(base_size = 12) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5))
```

The first funding round happen around the same time for most startups,
whether successful or not.

## Define Schema

As a single dataframe is used for this data science lifecycle, the need
for a schema is low. As the quality of the data seems quite high and the
dataframe is well structures, no schema is defined for this project.

## Anomaly Detection

An anomaly that was identified in the data analysis is that there is one
duplicate startup by name and id. This should be removed.

```{r}
# remove duplicate rows
train_data <- distinct(train_data)
```

```{r}
# remove duplicate rows
train_data_new <- distinct(train_data_new)
```

Other anomalies are missing values and outliers, which are handled now.

### Missing Values

At first, we need to find out how many missing values there are in which
column. They will be fixed in the feature engineering recipe later on.

```{r}
is.na(train_data) %>% 
    colSums()
```

There are 442 missing values in closed_at, probably because all
successful startups haven't closed, thus they don't have a closing date.
This feature is too tightly related to the prediction and thus needs to
be removed for the model. There are 112 missing values each for
age_first_milestone_year and age_last_milestone year. As there are only
a couple hundred startups in the data frame, the ratio of missing values
is to high to simply delete these rows. Instead, the missing values will
be replaced by the median. Having this many values replaced, the feature
importance for the model will decrease.

### Outlier Detection

In the data analysis, a few possible outliers have been called out and
are investigated now.

A longitude of higher than 50 and thus being located in Europe was
identified as an outlier. How many startups are located there?

```{sql connection=con}
SELECT COUNT(*)
FROM df_train
WHERE longitude > -50;
```

There are 4 startups in Europe in the dataframe. This is very small
amount, but this data frame will be expanded in the future through
information of our customers.

Another outlier are the negative first and last funding age. How many of
those are there in the data frame?

```{sql connection=con}
SELECT name, age_first_funding_year, age_last_funding_year
FROM df_train
WHERE age_first_funding_year < 0 OR age_last_funding_year < 0;
```

There are 35 startups with either a negative first or last funding age.
These are too many to remove them. If this projected was extended, the
root cause for a negative funding age should be identified. Another
realization from this analysis is that all startups that have a negative
last funding age also have a negative first funding age.

The first and last funding age were rather left skewed, thus the higher
values are evaluated to be outliers now.

```{sql connection=con}
SELECT name, age_first_funding_year, age_last_funding_year
FROM df_train
WHERE age_first_funding_year > 5 OR age_last_funding_year > 5;
```

The higher first and last funding age is no outlier, one third of the
startups have either their first or last funding after 5 years.

Another outlier are the negative first and last milestone age. How many
of those are there in the data frame?

```{sql connection=con}
SELECT name, age_first_milestone_year, age_last_milestone_year
FROM df_train
WHERE age_first_milestone_year < 0 OR age_last_milestone_year < 0;
```

Just as negative first and last funding age, there are 35 startups that
either have a negative first or last milestone age. Are those the same
startups?

```{sql connection=con}
SELECT COUNT(*)
FROM df_train
WHERE age_first_milestone_year < 0 OR age_last_milestone_year < 0 OR age_first_funding_year < 0 OR age_last_funding_year < 0;
```

The startups having negative milestone and funding ages differ mostly.
If it were the same, the count would be 35.

The variable relationships is left skewed. How many relationships are
considered as an outlier?

```{r}
# boxplot relationships
df_train %>%
  ggplot(aes(y = relationships)) +
  geom_boxplot()
```

Startups having more than 20 relationships are outliers.

The maximum funding_total_usd is tremendously higher than all other
funding amounts. Should this be removed?

```{r}
# boxplot funding_total_usd
df_train %>%
  ggplot(aes(y = funding_total_usd)) +
  geom_boxplot()
```

The other outliers are way lower than the maximum funding_total_usd,
that is why the startups with extremely high funding have to be removed
from the dataframe.

Funding rounds above 4 seem unreasonable and are thus investigated.

```{r}
# boxplot funding_rounds
df_train %>%
  ggplot(aes(y = funding_rounds)) +
  geom_boxplot()
```

There are only 3 startups with more than 6 funding rounds.

The earliest founding date in the 1980s is very early in comparison to
the rather right skewed values. Does it make sense to remove this?

```{r}
# histogram founded_at
df_train %>%
  ggplot(aes(x = founded_at)) +
  geom_histogram()
```

There is a huge gap between the earliest founded_at and the next in the
lat 1990s.

## Feature Engineering

Feature engineering is the process of using domain knowledge to extract
meaningful features from the data. The goal of this process is to keep
those features that improve the predictions from our model. For this a
pipeline is created that included the standardization of numeric
features and one hot encoding of categorical features. As the categories
and states are already one hot encoded, their categorical featured need
to be removed. The anomalies, like outliers, missing values or wrong
data types, will be fixed in the recipe in the next chapter.

# Model

This chapter describes the creation of a model to predict the success of
a startup. Several classification algorithms are optimized and compared
to select the best performing model.

## Select Algorithm

As the data is pre-categorized, having status as the to be predicted
label, supervised learning in form of classification is used. As the
data is not too complex, neural networks are not needed. Instead, two
common classification algorithms are compared: 
- Logistic Regression:Predicts a dependent variable by analyzing the relationship between one
or more existing independent variables. 
- Decision Trees: Uses tree-like model of decisions and their possible consequences.

## Model Training & Tuning

Models for all three algorithms are created and optimized in this
chapter.

### Feature Selection

At first, we need to decide which features we want to include in our
model. As identified in the correlation matrix, there is a strong
positive linear correlation between the first and last funding and
milestone age of startups. These highly correlated features are removed
and this is included in the recipe of Feature Engineering already.

```{r}
# For all startup information
df_rec <-
  recipe(status ~ ., data = train_data) %>%
  step_rm(closed_at, state_code, category_code, latitude, longitude, zip_code, city, name, id) %>% #remove unnecessary columns
  step_impute_median(age_first_milestone_year, age_last_milestone_year) %>% #replace NaN with median
  step_naomit(everything(), skip = TRUE) %>% #remove rows with NaN values (only as double check)
  step_mutate(founded_at = as.numeric(founded_at)) %>% #date to numeric
  step_mutate(first_funding_at = as.numeric(first_funding_at)) %>%
  step_mutate(last_funding_at = as.numeric(last_funding_at)) %>%
  step_corr(all_predictors(), threshold = 0.7, method = "spearman") %>% #remove highly correlated
  #step_outliers_maha(all_numeric(), -all_outcomes()) %>%
  #step_outliers_lookout(all_numeric(),-contains(r"(.outliers)"),-all_outcomes()) %>% 
  #step_outliers_remove(contains(r"(.outliers)")) %>% # remove outliers
  step_zv(all_numeric(), -all_outcomes()) %>% #remove all numeric that have zero variance
  step_normalize(all_numeric(), -all_outcomes()) #standardize numeric features
```

```{r}
# bake recipe
prepped_data <- 
  df_rec %>% # use the recipe object
  prep() %>% # perform the recipe on training data
  juice() # extract only the preprocessed dataframe 
```

```{r}
# Again for new startups
df_rec_new <-
  recipe(status ~ ., data = train_data_new) %>%
  step_rm(state_code, category_code, latitude, longitude, zip_code, city, name, id) %>% #remove unnecessary columns
  step_impute_median(age_first_milestone_year) %>% #replace NaN with median
  step_naomit(everything(), skip = TRUE) %>% #remove rows with NaN values (only as double check)
  step_mutate(founded_at = as.numeric(founded_at)) %>% #date to numeric
  step_mutate(first_funding_at = as.numeric(first_funding_at)) %>%
  step_corr(all_predictors(), threshold = 0.7, method = "spearman") %>% #remove highly correlated
  #step_outliers_maha(all_numeric(), -all_outcomes()) %>%
  #step_outliers_lookout(all_numeric(),-contains(r"(.outliers)"),-all_outcomes()) %>% 
  #step_outliers_remove(contains(r"(.outliers)")) %>% # remove outliers
  step_zv(all_numeric(), -all_outcomes()) %>% #remove all numeric that have zero variance
  step_normalize(all_numeric(), -all_outcomes()) #standardize numeric features
```

```{r}
# bake recipe
prepped_data_new <- 
  df_rec_new %>% # use the recipe object
  prep() %>% # perform the recipe on training data
  juice() # extract only the preprocessed dataframe 
```

Could not make outlier removel work, which could have a negative impact on the model performance.

From the training data another split is performed to create validation
data. This is used to verify the model before adding new data to avoid
overfitting of the model to the training data.

```{r}
# crossvalidation
set.seed(100)

cv_folds <-
 vfold_cv(train_data, 
          v = 5, 
          strata = status) 
```

```{r}
# crossvalidation also for new startups
set.seed(100)

cv_folds_new <-
 vfold_cv(train_data_new, 
          v = 5, 
          strata = status) 
```

### Training & Hyperparameter Tuning

In the first phase of the model process, the initial models are
generated and their performance is compared during model evaluation.
Besides the model specification, workflows need to be created to combine
the data preparation recipe with the model. As a third step, our
validation set (cv_folds) is fitted to estimate the performance of our
models afterwards.

```{r}
# For all startup information
# Logistic Regression
# model
log_spec <- # your model specification
  logistic_reg() %>%  # model type
  set_engine(engine = "glm") %>%  # model engine
  set_mode("classification") # model mode

# workflow pipeline
log_wflow <- # new workflow object
 workflow() %>% # use workflow function
 add_recipe(df_rec) %>%   # use the new recipe
 add_model(log_spec)   # add your model spec

# fit model with crossvalidation
log_res <- 
  log_wflow %>% 
  fit_resamples(
    resamples = cv_folds, 
    metrics = metric_set(yardstick::recall, yardstick::precision, accuracy),
    control = control_resamples(save_pred = TRUE)
    ) 
```

```{r}
# For all startup information
# Decision Tree
# model
xgb_spec <- 
  boost_tree() %>% 
  set_engine("xgboost") %>% 
  set_mode("classification") 

# workflow pipeline
xgb_wflow <-
 workflow() %>%
 add_recipe(df_rec) %>% 
 add_model(xgb_spec)

# fit model with crossvalidation
xgb_res <- 
  xgb_wflow %>% 
  fit_resamples(
    resamples = cv_folds, 
    metrics = metric_set(yardstick::recall, yardstick::precision, accuracy),
    control = control_resamples(save_pred = TRUE)
    ) 
```

The decision tree's parameters are tuned and a new model with those parameters is created.
The tuning takes very long to run, why it is removed from running after receiving the results once.

```{r}
# Hyperparameter tuning decision tree
# added #, because this takes almost an hour to run
#xgb_tune_spec <- 
# boost_tree(
#    trees = tune(),
#    min_n = tune(),
#    tree_depth = tune(),
#    learn_rate = tune(),
#    loss_reduction = tune(),
#    stop_iter = tune()
#  ) %>% 
#  set_engine("xgboost") %>% 
#  set_mode("classification")

#xgb_grid <- grid_regular(trees(),
#                         min_n(),
#                         tree_depth(),
#                         learn_rate(),
#                         loss_reduction(),
#                         stop_iter(),
#                         levels = 3)

#xgb_wflow_tune <- workflow() %>% 
# add_recipe(df_rec) %>%   
# add_model(xgb_tune_spec)

#xgb_res_tune <- xgb_wflow_tune %>% 
#  tune_grid(
#    resamples = cv_folds,
#    grid = xgb_grid
#  )

#best_tree <- xgb_res_tune %>%
#  select_best("accuracy")

#best_tree
```

The best configuration according to the accuracy for the boosted tree is 
- trees: 1000
- min_n: 2
- tree_depth: 1
- learn_rate: 0.1
- loss_reduction: 1e-10
- stop_iter: 3

Selecting the best tree by precision ended in an error, that is why accuracy was chosen.
This model configuration is now created to be compared to the non-tuned boosted trees and the linear regression model.

```{r}
# For all startup information
# Tuned Decision Tree
# model
xgb_spec_tuned <- 
  boost_tree(trees = 1, min_n = 2, tree_depth = 1, learn_rate = 0.1, loss_reduction = 1e-10, stop_iter = 3) %>% 
  set_engine("xgboost") %>% 
  set_mode("classification") 

# workflow pipeline
xgb_wflow_tuned <-
 workflow() %>%
 add_recipe(df_rec) %>% 
 add_model(xgb_spec_tuned)

# fit model with crossvalidation
xgb_res_tuned <- 
  xgb_wflow_tuned %>% 
  fit_resamples(
    resamples = cv_folds, 
    metrics = metric_set(yardstick::recall, yardstick::precision, accuracy),
    control = control_resamples(save_pred = TRUE)
    ) 
```

The same models are also created for a classification of new startups.

```{r}
# For new startups
# Logistic Regression
# model
log_spec_new <- # your model specification
  logistic_reg() %>%  # model type
  set_engine(engine = "glm") %>%  # model engine
  set_mode("classification") # model mode

# workflow pipeline
log_wflow_new <- # new workflow object
 workflow() %>% # use workflow function
 add_recipe(df_rec_new) %>%   # use the new recipe
 add_model(log_spec_new)   # add your model spec

# fit model with crossvalidation
log_res_new <- 
  log_wflow_new %>% 
  fit_resamples(
    resamples = cv_folds_new, 
    metrics = metric_set(yardstick::recall, yardstick::precision, accuracy),
    control = control_resamples(save_pred = TRUE)
    ) 
```

```{r}
# For new startups
# Decision Tree
# model
xgb_spec_new <- 
  boost_tree() %>% 
  set_engine("xgboost") %>% 
  set_mode("classification") 

# workflow pipeline
xgb_wflow_new <-
 workflow() %>%
 add_recipe(df_rec_new) %>% 
 add_model(xgb_spec_new)

# fit model with crossvalidation
xgb_res_new <- 
  xgb_wflow_new %>% 
  fit_resamples(
    resamples = cv_folds_new, 
    metrics = metric_set(yardstick::recall, yardstick::precision, accuracy),
    control = control_resamples(save_pred = TRUE)
    ) 
```

```{r}
# For new startups
# Hyperparameter tuning decision tree
# added #, because this takes almost an hour to run
#xgb_tune_spec_new <- 
#  boost_tree(
#    trees = tune(),
#    min_n = tune(),
#    tree_depth = tune(),
#    learn_rate = tune(),
#    loss_reduction = tune(),
#    stop_iter = tune()
#  ) %>% 
#  set_engine("xgboost") %>% 
#  set_mode("classification")

#xgb_grid_new <- grid_regular(trees(),
#                         min_n(),
#                         tree_depth(),
#                         learn_rate(),
#                         loss_reduction(),
#                         stop_iter(),
#                         levels = 3)

#xgb_wflow_tune_new <- workflow() %>% 
# add_recipe(df_rec_new) %>%   
# add_model(xgb_tune_spec_new)

#xgb_res_tune_new <- xgb_wflow_tune_new %>% 
#  tune_grid(
#    resamples = cv_folds_new,
#    grid = xgb_grid_new
#  )

#best_tree_new <- xgb_res_tune_new %>%
#  select_best("accuracy")

#best_tree_new
```

The best configuration according to the accuracy for the boosted tree for new startups is 
- trees: 1000
- min_n: 21
- tree_depth: 1
- learn_rate: 0.1
- loss_reduction: 1e-10
- stop_iter: 3

This model configuration is now created to be compared to the non-tuned boosted trees and the linear regression model.

```{r}
# For all startup information
# Tuned Decision Tree
# model
xgb_spec_new_tuned <- 
  boost_tree(trees = 1, min_n = 21, tree_depth = 1, learn_rate = 0.1, loss_reduction = 1e-10, stop_iter = 3) %>% 
  set_engine("xgboost") %>% 
  set_mode("classification") 

# workflow pipeline
xgb_wflow_new_tuned <-
 workflow() %>%
 add_recipe(df_rec) %>% 
 add_model(xgb_spec_new_tuned)

# fit model with crossvalidation
xgb_res_new_tuned <- 
  xgb_wflow_new_tuned %>% 
  fit_resamples(
    resamples = cv_folds, 
    metrics = metric_set(yardstick::recall, yardstick::precision, accuracy),
    control = control_resamples(save_pred = TRUE)
    ) 
```


### Evaluation

Now the results of the models are compared and their performance
evaluated to decide which model is the best for our startup success
prediction use case. An in-detail walk through of the metrics is done with the logistic regression for all startup information. The other models are then compared to each other through their accuracy and precision.

Average performance over all folds for logistic regression.

```{r}
log_res %>% 
  collect_metrics(summarize = TRUE)
```

We have a closer look at three metrics: Accuracy, precision and recall.
The accuracy rates the overall correct classifications. Precision
describes how much of the samples, which have been classified as
positive are actual positive. And recall describes how much of the true
positive samples has been classified as positive.

The logistic regression model has an accuracy of 73%. 77% of the
startups that it predicted to be successful, are actually successful and
85% of true positive samples habe been classified as positive.

Performance for every single fold for logistic regression.

```{r}
log_res %>% 
  collect_metrics(summarize = FALSE)
```

The range for all metrics is not too big, but rather similar for every
fold.

Now, we obtain the actual model predictions to compare the amount of
correct and false predictions.

```{r}
log_pred <- 
  log_res %>%
  collect_predictions()
```

A confusion matrix is created to compare the amounts of predictions. How
to read a classification matrix:

<img src="https://www.researchgate.net/publication/336402347/figure/fig3/AS:812472659349505@1570719985505/Calculation-of-Precision-Recall-and-Accuracy-in-the-confusion-matrix.ppm"/>

For the StandardScaler Pipeline this means that in 5 cases label 0 was
predicted correctly, 22 times as label 1 and 59 times as label 2. Lables
3 and 4 were not predicted correctly at all.

From these numbers, various metrics can be calculated.

Accuracy: The rate of overall correct classifications.

$$
ACC=\frac{TP+TN}{FP+FN+TP+TN}
$$

Precision: How much of the samples, which have been classified as
positive are actual positive

$$
PRE=\frac{TP}{FP+TP}
$$

Recall: How much of the true positive samples has been classified as
positive

$$
REC=\frac{TP}{FN+TP}
$$

For the prediction of startup success, we want the number of False
Positives to be as little as possible, because these mean an investment
gone wrong and lots of money being lost for the investors and thus focus
on precision.

```{r}
log_pred %>%
  conf_mat(status, .pred_class) 
```

The confusion matrix can also be visualized to make reading it easier.

```{r}
log_pred %>%
  conf_mat(status, .pred_class) %>% 
  autoplot(type = "heatmap")
```

382 startups are True Positives, as they have been predicted to be
acquired and have also been acquired in reality. On the other hand, 123
startups that closed have also been predicted to close, those are the
True Negatives. There are 65 False Negatives, that have been acquired,
but were predicted to close. The fourth kind are the False Positives,
those have closed in reality, but were predicted to be acquired. There
are 121 False Positives in this logistic regression and this is the
number that should be as low as possible.

Instead of looking at the metrics one by one, we compare them now. The first comparison is for all stratup startup information, afterwards the same is done for new startups.

```{r}
# all startup information
# compare models
log_metrics <- 
  log_res %>% 
  collect_metrics(summarise = TRUE) %>%
  mutate(model = "Logistic Regression") # add the name of the model to every row

xgb_metrics <- 
  xgb_res %>% 
  collect_metrics(summarise = TRUE) %>%
  mutate(model = "XGBoost")

xgb_tuned_metrics <- 
  xgb_res_tuned %>% 
  collect_metrics(summarise = TRUE) %>%
  mutate(model = "XGBoost Tuned")

# create dataframe with all models
model_compare <- bind_rows(log_metrics,
                           xgb_metrics,
                           xgb_tuned_metrics) 

# change data structure
model_comp <- 
  model_compare %>% 
  select(model, .metric, mean, std_err) %>% 
  pivot_wider(names_from = .metric, values_from = c(mean, std_err)) 

# show mean accuracy for every model
model_comp %>% 
  arrange(mean_accuracy) %>% 
  mutate(model = fct_reorder(model, mean_accuracy)) %>% # order results
  ggplot(aes(model, mean_accuracy, fill=model)) +
  geom_col() +
  coord_flip() +
  scale_fill_brewer(palette = "Blues") +
   geom_text(
     size = 3,
     aes(label = round(mean_accuracy, 2)),
     vjust = 1
  )

# show mean precision per model
model_comp %>% 
  arrange(mean_precision) %>% 
  mutate(model = fct_reorder(model, mean_precision)) %>%
  ggplot(aes(model, mean_precision, fill=model)) +
  geom_col() +
  coord_flip() +
  scale_fill_brewer(palette = "Blues") + 
     geom_text(
     size = 3,
     aes(label = round(mean_precision, 2)),
     vjust = 1
  )
```

The mean accuracy over 5 folds is lowest using logistic regression with 73%. The tuned decision trees perform a little better with an accuracy of 74% and the best accuracy of 76% has the non-tuned decision tree.
The mean precision over 5 folds is lowest using logistic regression or tuned decision trees with 76%. The non-tuned decision trees perform a little better with a precision of 78%.
The non-tuned gradient boosted decision tree is the model with the best performance. The
objective for a good model, is supposed to be 80% for accuracy and
precision and those are not quite reached, but at least it is close.
It is odd that the non-tuned decision tree performs better than the tuned one. An explanation could not be found.

The comparison is also done for new startups. As less features are inserted to the model, a worse performance is expected for all models.
```{r}
# new startups
# compare models
log_metrics_new <- 
  log_res_new %>% 
  collect_metrics(summarise = TRUE) %>%
  mutate(model = "Logistic Regression") # add the name of the model to every row

xgb_metrics_new <- 
  xgb_res_new %>% 
  collect_metrics(summarise = TRUE) %>%
  mutate(model = "XGBoost")

xgb_metrics_new_tuned <- 
  xgb_res_new_tuned %>% 
  collect_metrics(summarise = TRUE) %>%
  mutate(model = "XGBoost Tuned")

# create dataframe with all models
model_compare_new <- bind_rows(log_metrics_new,
                               xgb_metrics_new,
                               xgb_metrics_new_tuned) 

# change data structure
model_comp_new <- 
  model_compare_new %>% 
  select(model, .metric, mean, std_err) %>% 
  pivot_wider(names_from = .metric, values_from = c(mean, std_err)) 

# show mean accuracy for every model
model_comp_new %>% 
  arrange(mean_accuracy) %>% 
  mutate(model = fct_reorder(model, mean_accuracy)) %>% # order results
  ggplot(aes(model, mean_accuracy, fill=model)) +
  geom_col() +
  coord_flip() +
  scale_fill_brewer(palette = "Blues") +
   geom_text(
     size = 3,
     aes(label = round(mean_accuracy, 2)),
     vjust = 1
  )

# show mean precision per model
model_comp_new %>% 
  arrange(mean_precision) %>% 
  mutate(model = fct_reorder(model, mean_precision)) %>%
  ggplot(aes(model, mean_precision, fill=model)) +
  geom_col() +
  coord_flip() +
  scale_fill_brewer(palette = "Blues") + 
     geom_text(
     size = 3,
     aes(label = round(mean_precision, 2)),
     vjust = 1
  )
```

The mean accuracy over 5 folds is lowest using logistic regression and non-tuned decision trees with 72%. The best accuracy has been achieved by the tuned decision tree with 74%.
The mean precision over 5 folds is the same of  76% for all three models.
The tuned gradient boosted decision tree is the model with the best performance and the overall performance of the algorithms is not significantly worse for new startups than for those where all information is included. 
The objective for a good model, is supposed to be 80% for accuracy and
precision and those are not quite reached, but at least it is close.

## Evaluate Model

The best model is evaluated on its errors to understand why it makes them and how to fix them.

```{r}
# create prediction variable
xgb_pred <- 
  xgb_res %>%
  collect_predictions()

xgb_pred_new <- 
  xgb_res_new_tuned %>%
  collect_predictions()
```

```{r}
# obtain the 10 wrongest predictions
#wrongest_predictions <- 
#  xgb_pred %>% 
#  mutate(residual = status - .pred_class) %>% 
#  arrange(desc(abs(residual))) %>% 
#  slice_head(n = 10)

# take a look at the observations
#errors <- 
#  train_data %>% 
#  dplyr::slice(wrongest_predictions$.row) 
```

Does not seem to work, because status and .pre_class are factors and not numeric. 

# Deployment

The last chapter describes the deployment of the final model.

## Validate Model

The best performing models are now evaluated on
test data. At first the best performing model for all startup information - Decision Trees - is validated.

```{r}
last_fit_xgb <- last_fit(xgb_wflow, 
                        split = data_split,
                        metrics = metric_set(yardstick::recall, yardstick::precision, accuracy)
                        )
```

```{r}
last_fit_xgb %>% 
  collect_metrics()
```

The results on the test data are even a little better than on the train data. The Decision Tree reaches an accuracy of 79% and a precision of 80%. The recall, which is not as important for this use case, is even at 91%.
The performance on test data is very similar to train data, which means that the model performs well on new data and is not overfitted to the training data.

Which features are the most important for startup success prediction?

```{r}
# feature importance
last_fit_xgb %>% 
  pluck(".workflow", 1) %>%   
  pull_workflow_fit() %>% 
  vip(num_features = 10)
```

The most important features are the number of relationships, the total funding and the age at first funding.

The same validation is done for the new startups with the best performing model being Tuned Decision Trees.

```{r}
#last_fit_xgb_new <- last_fit(xgb_wflow_new_tuned, 
#                             split = data_split_new,
#                             metrics = metric_set(yardstick::recall, yardstick::precision, accuracy)
#                             )
```

```{r}
#last_fit_xgb_new %>% 
#  collect_metrics()
```


```{r}
# feature importance
#last_fit_xgb_new %>% 
#  pluck(".workflow", 1) %>%   
#  pull_workflow_fit() %>% 
#  vip(num_features = 10)
```

Unfortunately, there is an error in the first step about missing columns that could not be removed.

## Deploy Model

There are three major options to deploy: Posit Connect, Tensorflow and Plumber.

To deploy the model a connection to Posit Connect was tried to be established. This did not work, but instead received this error when trying to install through th terminal as described [here](https://docs.posit.co/rsc/installation/):

(base) helena.schick@NANZ204 ~ % curl -Lo rsc-installer.sh https://cdn.rstudio.com/connect/installer/installer-v6.4.0.sh
sudo -E bash ./rsc-installer.sh 2023.03.0

  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 23.7M  100 23.7M    0     0  7742k      0  0:00:03  0:00:03 --:--:-- 7753k
Password:
helena.schick is not in the sudoers file.  This incident will be reported.

I tried this code though before realizing that I cannot make it work:

> # Fit models
xgb_fit <- fit(xgb_wflow, data = train_data)
xgb_fit
model_name <- "xgb_startup_success_prediction"
pin_name <- glue("helena.schick/{model_name}")
date_metadata <- list(
  train_dates = c(
    as.character(min(train_data$founded_at)), 
    as.character(max(train_data$founded_at))
  ),
  test_dates = c(
    as.character(min(test_data$founded_at)), 
    as.character(max(test_data$founded_at))
  )
)
print(date_metadata)
# Create the vetiver model.
v <- vetiver_model(
  xgb_fit, 
  model_name,
  versioned = TRUE,
  save_ptype = train_data %>%
    head(1)  %>%
    select(-status),
  metadata = date_metadata
)
v

># Use RStudio Connect as a board.
board <- pins::board_connect()
# Write the model to the board.
board %>%
 vetiver_pin_write(vetiver_model = v)

># Add server
rsconnect::addServer(
  url = "https://colorado.rstudio.com/rsc/__api__",
  name = "colorado"
)
# Add account
if (FALSE) {
rsconnect::connectApiUser(
  account = "helena.schick",
  server = "colorado",
  apiKey = Sys.getenv("CONNECT_API_KEY"),
)
}
# Deploy to Connect
vetiver_deploy_rsconnect(
  board = board,
  name = pin_name,
  appId = "11314",
  launch.browser = FALSE,
  appTitle = "Startup Predict - Model - API",
  predict_args = list(debug = FALSE),
  account = "helena.schick",
  server =  "colorado"
)

Thus, RStudio's Model Management could not be used.

As I have a MacBook with M1, tensorflow does not properly run there. This is why, I did not try TFX pipelines.

The last option is plumber. This has to be run in a R Script, see plumber.R.
The model has to be saved, so it can be inserted in the R Script.

```{r}
saveRDS(last_fit_xgb, "model.rds")
```

Also deploy as a dashboard in shiny from separate .Rmd file.

## Serve Model

For this project, serving the model cannot be conducted. If this became a real business case though, serving the model would be implemented through information of new startups. The model can learn from adding this new information.

## Monitor Model

The inability to conduct this step of monitoring the model applies as to serving the model. Over the course of time and when ingesting new information, the performance of the model would be monitored. Ideally, the model performance increases through additional data. If this weren't the case, the hyperparameters might need to be adjusted or even a better performing model needs to be evaluated.

# Conclusion

This project's objective was the prediction of the success of startups to support investors making more reliable investment decision and reducing their risk. To ensure all steps of data analysis and model creation are done for the best results, the data science lifecycle was used for reference and coding is done in R and SQL.

The first step, was to introduce the business case for startups success prediction in more detail and set model performance objectives. A good classification model for this use case is defined as an accuracy and precision of at least 80%.

Before creating the model, the given data is split into train and test data and being analyzed. The key findings from the data analysis are:

- Most successful startups are in the categories software and web.
- Most successful startups are from California.
- Successful startups have more relationships on average.
- The age of the startups at first funding does not differ between acquired and closed startups.

After the feature engineering, the models can be created. Two classification models, linear regression and decision trees, are optimized and their performance based on the defined metrics compared.
On one hand, this is done with data containing all relevant given information about the startups and on the other hand, only using the features that are given for new startups that are upto their first funding round.

The best performing model with all information is Decision Trees with an accuracy of 76% and precision of 78%. The best performing model for new startups is Tuned Decision Trees with an accuracy of 74% and precision of 76%. Both are not considered good models as they do not perform to the given success metrics for training and test data. Their performance on test and training data is very similar, which means that the model performs well on new data and is not overfitted to the training data.

In the end, the model is deployed to a Web API that anyone can access it through the link instead of opening this entire file. If more data on startups was inserted, the deployed model could be served and monitored over time.

During the implementation of this project, some challenges have been faced: The first challenge was that the removal of outliers in the recipe does not work, through this the model performance could increase though. Afterwards, the hyperparameter optimization for the decision tree takes very long to run (more than an hour) and the tuned decision tree for new startups could not be validated. Another challenge was the deployment of the model, as RStudio's Model Management and tensorflow did not work, but the third option plumber works, but does not have pretty user interface.

All in all, this project achieved its learning objectives to become confident in the programming languages R and SQL. In addition, its content created transparency over the success factors of startups and the set metrics of the classification model were almost achieved. 
