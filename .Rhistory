df %>%
select(all_of(features))
# list of numeric feature names
feat_num <-
X %>%
select(where(is.numeric)) %>%
names()
# list of categorical feature names
feat_cat <-
X %>%
select(!where(is.numeric)) %>%
names()
# create response for data splitting
y <-
df %>%
select(all_of(y_label))
# Fix the random numbers by setting the seed
# This enables the analysis to be reproducible
set.seed(42)
# Put 3/4 of the data into the training set
data_split <- initial_split(df,
prop = 3/4,
strata = status,
breaks = 4)
# Create dataframes for the two sets:
train_data <- training(data_split)
test_data <- testing(data_split)
# Same for df_new
set.seed(42)
# Put 3/4 of the data into the training set
data_split_new <- initial_split(df_new,
prop = 3/4,
strata = status,
breaks = 4)
# Create dataframes for the two sets:
train_data_new <- training(data_split_new)
test_data_new <- testing(data_split_new)
df_train <- train_data
skim(df_train)
# Connection to database SQlite
con <- dbConnect(RSQLite::SQLite(), ":memory:")
# Write data "df_train" into database
dbWriteTable(con, "df_train", df_train)
# List tables
dbListTables(con)
n <- 10  # Number of most frequent levels to show
for (i in feat_cat){
counts <- df_train %>% count(!!sym(i)) %>% arrange(desc(n)) %>% head(n)
p <- ggplot(counts, aes_string(x=i, y="n")) +
geom_bar(stat="identity")
plot(p)
}
# Removing one hot encoded features
df_numeric <- df_train %>%
select(all_of(feat_num))
df_numeric <- df_numeric %>%
select(-is_CA, -is_NY, -is_MA, -is_TX, -is_otherstate, -is_software, -is_web, -is_mobile, -is_enterprise, -is_advertising, -is_gamesvideo, -is_ecommerce, -	is_biotech, -is_consulting, -is_enterprise, -is_othercategory, -has_VC, -has_angel, -has_roundA, -has_roundB, -has_roundC, -has_roundD, -is_top500)
# Create histograms using ggplot2
hist_list <- lapply(names(df_numeric), function(col) {
ggplot(df_numeric, aes(x = .data[[col]])) +
geom_histogram(bins = 10) +
ggtitle(paste0(col))
})
# Arrange histograms in a grid
library(patchwork)
hist_list[[1]] + hist_list[[2]] + hist_list[[3]] + plot_layout(ncol = 3)
hist_list[[4]] + hist_list[[5]] + hist_list[[6]] + plot_layout(ncol = 3)
hist_list[[7]] + hist_list[[8]] + hist_list[[9]] + plot_layout(ncol = 3)
df_numeric %>%
vis_cor(cor_method = "spearman", na_action = "pairwise.complete.obs")
df_numeric %>%
ggscatmat() +
theme(axis.text.x = element_text(angle = 90, vjust = -1))
df_train %>%
ggplot(aes(x = status, y = relationships, color = status)) +
geom_boxplot() +
labs(title = "Relationships, if successful or not") +
theme_bw(base_size = 12) +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5))
df_train %>%
ggplot(aes(x = relationships, y = funding_rounds, group = status, color = status)) +
geom_point() +
theme_classic(base_size = 12) +
ggtitle("Correlation of status, relationships and funding_rounds") +
theme(legend.title = element_blank())
world_map <- map_data("world")
ggplot(df_train, aes(x = longitude, y = latitude, color = status)) +
geom_point() +
geom_path(data = world_map, aes(x = long, y = lat, group = group), color = "gray50") +
labs(x = "Longitude", y = "Latitude", title = "Startup Locations")
library(ggmap)
qmplot(x = longitude,
y = latitude,
data = df_train,
geom = "point",
color = status,
alpha = 0.4) +
scale_alpha(guide = 'none') # don't show legend for alpha
df_train %>%
ggplot(aes(x = status, y = age_first_funding_year, color = status)) +
geom_boxplot() +
labs(title = "Age first funding, if successful or not") +
theme_bw(base_size = 12) +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5))
# remove duplicate rows
train_data <- distinct(train_data)
# remove duplicate rows
train_data_new <- distinct(train_data_new)
is.na(train_data) %>%
colSums()
# boxplot relationships
df_train %>%
ggplot(aes(y = relationships)) +
geom_boxplot()
# boxplot funding_total_usd
df_train %>%
ggplot(aes(y = funding_total_usd)) +
geom_boxplot()
# boxplot funding_rounds
df_train %>%
ggplot(aes(y = funding_rounds)) +
geom_boxplot()
# histogram founded_at
df_train %>%
ggplot(aes(x = founded_at)) +
geom_histogram()
# For all startup information
df_rec <-
recipe(status ~ ., data = train_data) %>%
step_rm(closed_at, state_code, category_code, latitude, longitude, zip_code, city, name, id) %>% #remove unnecessary columns
step_impute_median(age_first_milestone_year, age_last_milestone_year) %>% #replace NaN with median
step_naomit(everything(), skip = TRUE) %>% #remove rows with NaN values (only as double check)
step_mutate(founded_at = as.numeric(founded_at)) %>% #date to numeric
step_mutate(first_funding_at = as.numeric(first_funding_at)) %>%
step_mutate(last_funding_at = as.numeric(last_funding_at)) %>%
step_corr(all_predictors(), threshold = 0.7, method = "spearman") %>% #remove highly correlated
#step_outliers_maha(all_numeric(), -all_outcomes()) %>%
#step_outliers_lookout(all_numeric(),-contains(r"(.outliers)"),-all_outcomes()) %>%
#step_outliers_remove(contains(r"(.outliers)")) %>% # remove outliers
step_zv(all_numeric(), -all_outcomes()) %>% #remove all numeric that have zero variance
step_normalize(all_numeric(), -all_outcomes()) #standardize numeric features
# bake recipe
prepped_data <-
df_rec %>% # use the recipe object
prep() %>% # perform the recipe on training data
juice() # extract only the preprocessed dataframe
# Again for new startups
df_rec_new <-
recipe(status ~ ., data = train_data_new) %>%
step_rm(state_code, category_code, latitude, longitude, zip_code, city, name, id) %>% #remove unnecessary columns
step_impute_median(age_first_milestone_year) %>% #replace NaN with median
step_naomit(everything(), skip = TRUE) %>% #remove rows with NaN values (only as double check)
step_mutate(founded_at = as.numeric(founded_at)) %>% #date to numeric
step_mutate(first_funding_at = as.numeric(first_funding_at)) %>%
step_corr(all_predictors(), threshold = 0.7, method = "spearman") %>% #remove highly correlated
#step_outliers_maha(all_numeric(), -all_outcomes()) %>%
#step_outliers_lookout(all_numeric(),-contains(r"(.outliers)"),-all_outcomes()) %>%
#step_outliers_remove(contains(r"(.outliers)")) %>% # remove outliers
step_zv(all_numeric(), -all_outcomes()) %>% #remove all numeric that have zero variance
step_normalize(all_numeric(), -all_outcomes()) #standardize numeric features
# bake recipe
prepped_data_new <-
df_rec_new %>% # use the recipe object
prep() %>% # perform the recipe on training data
juice() # extract only the preprocessed dataframe
# crossvalidation
set.seed(100)
cv_folds <-
vfold_cv(train_data,
v = 5,
strata = status)
# crossvalidation also for new startups
set.seed(100)
cv_folds_new <-
vfold_cv(train_data_new,
v = 5,
strata = status)
# For all startup information
# Logistic Regression
# model
log_spec <- # your model specification
logistic_reg() %>%  # model type
set_engine(engine = "glm") %>%  # model engine
set_mode("classification") # model mode
# workflow pipeline
log_wflow <- # new workflow object
workflow() %>% # use workflow function
add_recipe(df_rec) %>%   # use the new recipe
add_model(log_spec)   # add your model spec
# fit model with crossvalidation
log_res <-
log_wflow %>%
fit_resamples(
resamples = cv_folds,
metrics = metric_set(yardstick::recall, yardstick::precision, accuracy),
control = control_resamples(save_pred = TRUE)
)
# For all startup information
# Decision Tree
# model
xgb_spec <-
boost_tree() %>%
set_engine("xgboost") %>%
set_mode("classification")
# workflow pipeline
xgb_wflow <-
workflow() %>%
add_recipe(df_rec) %>%
add_model(xgb_spec)
# fit model with crossvalidation
xgb_res <-
xgb_wflow %>%
fit_resamples(
resamples = cv_folds,
metrics = metric_set(yardstick::recall, yardstick::precision, accuracy),
control = control_resamples(save_pred = TRUE)
)
# Hyperparameter tuning decision tree
# added #, because this takes almost an hour to run
#xgb_tune_spec <-
# boost_tree(
#    trees = tune(),
#    min_n = tune(),
#    tree_depth = tune(),
#    learn_rate = tune(),
#    loss_reduction = tune(),
#    stop_iter = tune()
#  ) %>%
#  set_engine("xgboost") %>%
#  set_mode("classification")
#xgb_grid <- grid_regular(trees(),
#                         min_n(),
#                         tree_depth(),
#                         learn_rate(),
#                         loss_reduction(),
#                         stop_iter(),
#                         levels = 3)
#xgb_wflow_tune <- workflow() %>%
# add_recipe(df_rec) %>%
# add_model(xgb_tune_spec)
#xgb_res_tune <- xgb_wflow_tune %>%
#  tune_grid(
#    resamples = cv_folds,
#    grid = xgb_grid
#  )
#best_tree <- xgb_res_tune %>%
#  select_best("accuracy")
#best_tree
# For all startup information
# Tuned Decision Tree
# model
xgb_spec_tuned <-
boost_tree(trees = 1, min_n = 2, tree_depth = 1, learn_rate = 0.1, loss_reduction = 1e-10, stop_iter = 3) %>%
set_engine("xgboost") %>%
set_mode("classification")
# workflow pipeline
xgb_wflow_tuned <-
workflow() %>%
add_recipe(df_rec) %>%
add_model(xgb_spec_tuned)
# fit model with crossvalidation
xgb_res_tuned <-
xgb_wflow_tuned %>%
fit_resamples(
resamples = cv_folds,
metrics = metric_set(yardstick::recall, yardstick::precision, accuracy),
control = control_resamples(save_pred = TRUE)
)
# For new startups
# Logistic Regression
# model
log_spec_new <- # your model specification
logistic_reg() %>%  # model type
set_engine(engine = "glm") %>%  # model engine
set_mode("classification") # model mode
# workflow pipeline
log_wflow_new <- # new workflow object
workflow() %>% # use workflow function
add_recipe(df_rec_new) %>%   # use the new recipe
add_model(log_spec_new)   # add your model spec
# fit model with crossvalidation
log_res_new <-
log_wflow_new %>%
fit_resamples(
resamples = cv_folds_new,
metrics = metric_set(yardstick::recall, yardstick::precision, accuracy),
control = control_resamples(save_pred = TRUE)
)
# For new startups
# Decision Tree
# model
xgb_spec_new <-
boost_tree() %>%
set_engine("xgboost") %>%
set_mode("classification")
# workflow pipeline
xgb_wflow_new <-
workflow() %>%
add_recipe(df_rec_new) %>%
add_model(xgb_spec_new)
# fit model with crossvalidation
xgb_res_new <-
xgb_wflow_new %>%
fit_resamples(
resamples = cv_folds_new,
metrics = metric_set(yardstick::recall, yardstick::precision, accuracy),
control = control_resamples(save_pred = TRUE)
)
# For new startups
# Hyperparameter tuning decision tree
# added #, because this takes almost an hour to run
#xgb_tune_spec_new <-
#  boost_tree(
#    trees = tune(),
#    min_n = tune(),
#    tree_depth = tune(),
#    learn_rate = tune(),
#    loss_reduction = tune(),
#    stop_iter = tune()
#  ) %>%
#  set_engine("xgboost") %>%
#  set_mode("classification")
#xgb_grid_new <- grid_regular(trees(),
#                         min_n(),
#                         tree_depth(),
#                         learn_rate(),
#                         loss_reduction(),
#                         stop_iter(),
#                         levels = 3)
#xgb_wflow_tune_new <- workflow() %>%
# add_recipe(df_rec_new) %>%
# add_model(xgb_tune_spec_new)
#xgb_res_tune_new <- xgb_wflow_tune_new %>%
#  tune_grid(
#    resamples = cv_folds_new,
#    grid = xgb_grid_new
#  )
#best_tree_new <- xgb_res_tune_new %>%
#  select_best("accuracy")
#best_tree_new
# For all startup information
# Tuned Decision Tree
# model
xgb_spec_new_tuned <-
boost_tree(trees = 1, min_n = 21, tree_depth = 1, learn_rate = 0.1, loss_reduction = 1e-10, stop_iter = 3) %>%
set_engine("xgboost") %>%
set_mode("classification")
# workflow pipeline
xgb_wflow_new_tuned <-
workflow() %>%
add_recipe(df_rec) %>%
add_model(xgb_spec_new_tuned)
# fit model with crossvalidation
xgb_res_new_tuned <-
xgb_wflow_new_tuned %>%
fit_resamples(
resamples = cv_folds,
metrics = metric_set(yardstick::recall, yardstick::precision, accuracy),
control = control_resamples(save_pred = TRUE)
)
log_res %>%
collect_metrics(summarize = TRUE)
log_res %>%
collect_metrics(summarize = FALSE)
log_pred <-
log_res %>%
collect_predictions()
log_pred %>%
conf_mat(status, .pred_class)
log_pred %>%
conf_mat(status, .pred_class) %>%
autoplot(type = "heatmap")
# all startup information
# compare models
log_metrics <-
log_res %>%
collect_metrics(summarise = TRUE) %>%
mutate(model = "Logistic Regression") # add the name of the model to every row
xgb_metrics <-
xgb_res %>%
collect_metrics(summarise = TRUE) %>%
mutate(model = "XGBoost")
xgb_tuned_metrics <-
xgb_res_tuned %>%
collect_metrics(summarise = TRUE) %>%
mutate(model = "XGBoost Tuned")
# create dataframe with all models
model_compare <- bind_rows(log_metrics,
xgb_metrics,
xgb_tuned_metrics)
# change data structure
model_comp <-
model_compare %>%
select(model, .metric, mean, std_err) %>%
pivot_wider(names_from = .metric, values_from = c(mean, std_err))
# show mean accuracy for every model
model_comp %>%
arrange(mean_accuracy) %>%
mutate(model = fct_reorder(model, mean_accuracy)) %>% # order results
ggplot(aes(model, mean_accuracy, fill=model)) +
geom_col() +
coord_flip() +
scale_fill_brewer(palette = "Blues") +
geom_text(
size = 3,
aes(label = round(mean_accuracy, 2)),
vjust = 1
)
# show mean precision per model
model_comp %>%
arrange(mean_precision) %>%
mutate(model = fct_reorder(model, mean_precision)) %>%
ggplot(aes(model, mean_precision, fill=model)) +
geom_col() +
coord_flip() +
scale_fill_brewer(palette = "Blues") +
geom_text(
size = 3,
aes(label = round(mean_precision, 2)),
vjust = 1
)
# new startups
# compare models
log_metrics_new <-
log_res_new %>%
collect_metrics(summarise = TRUE) %>%
mutate(model = "Logistic Regression") # add the name of the model to every row
xgb_metrics_new <-
xgb_res_new %>%
collect_metrics(summarise = TRUE) %>%
mutate(model = "XGBoost")
xgb_metrics_new_tuned <-
xgb_res_new_tuned %>%
collect_metrics(summarise = TRUE) %>%
mutate(model = "XGBoost Tuned")
# create dataframe with all models
model_compare_new <- bind_rows(log_metrics_new,
xgb_metrics_new,
xgb_metrics_new_tuned)
# change data structure
model_comp_new <-
model_compare_new %>%
select(model, .metric, mean, std_err) %>%
pivot_wider(names_from = .metric, values_from = c(mean, std_err))
# show mean accuracy for every model
model_comp_new %>%
arrange(mean_accuracy) %>%
mutate(model = fct_reorder(model, mean_accuracy)) %>% # order results
ggplot(aes(model, mean_accuracy, fill=model)) +
geom_col() +
coord_flip() +
scale_fill_brewer(palette = "Blues") +
geom_text(
size = 3,
aes(label = round(mean_accuracy, 2)),
vjust = 1
)
# show mean precision per model
model_comp_new %>%
arrange(mean_precision) %>%
mutate(model = fct_reorder(model, mean_precision)) %>%
ggplot(aes(model, mean_precision, fill=model)) +
geom_col() +
coord_flip() +
scale_fill_brewer(palette = "Blues") +
geom_text(
size = 3,
aes(label = round(mean_precision, 2)),
vjust = 1
)
# create prediction variable
xgb_pred <-
xgb_res %>%
collect_predictions()
xgb_pred_new <-
xgb_res_new_tuned %>%
collect_predictions()
# obtain the 10 wrongest predictions
#wrongest_predictions <-
#  xgb_pred %>%
#  mutate(residual = status - .pred_class) %>%
#  arrange(desc(abs(residual))) %>%
#  slice_head(n = 10)
# take a look at the observations
#errors <-
#  train_data %>%
#  dplyr::slice(wrongest_predictions$.row)
last_fit_xgb <- last_fit(xgb_wflow,
split = data_split,
metrics = metric_set(yardstick::recall, yardstick::precision, accuracy)
)
last_fit_xgb %>%
collect_metrics()
# feature importance
last_fit_xgb %>%
pluck(".workflow", 1) %>%
pull_workflow_fit() %>%
vip(num_features = 10)
#last_fit_xgb_new <- last_fit(xgb_wflow_new_tuned,
#                             split = data_split_new,
#                             metrics = metric_set(yardstick::recall, yardstick::precision, accuracy)
#                             )
#last_fit_xgb_new %>%
#  collect_metrics()
# feature importance
#last_fit_xgb_new %>%
#  pluck(".workflow", 1) %>%
#  pull_workflow_fit() %>%
#  vip(num_features = 10)
saveRDS(last_fit_xgb, "model.rds")
plumber::plumb(file='plumber.R')$run()
plumber::plumb(file='plumber.R')$run()
