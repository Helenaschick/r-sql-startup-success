split = data_split,
metrics = metric_set(yardstick::recall, yardstick::precision, accuracy)
)
# feature importance
last_fit_xgb %>%
pluck(".workflow", 1) %>%
pull_workflow_fit() %>%
vip(num_features = 10
model <- readRDS("model.rds")
last_fit_xgb <- last_fit(xgb_wflow,
split = data_split,
metrics = metric_set(yardstick::recall, yardstick::precision, accuracy)
)
# feature importance
last_fit_xgb %>%
pluck(".workflow", 1) %>%
pull_workflow_fit() %>%
vip(num_features = 10)
?set_mode
add_model
?add_model
?fit_resamples
?metric_Set
?metric_set
library(flexdashboard)
library(ggplot2)
library(plotly)
library(plyr)
library(readr)
library(leaflet)
library(visdat)
library(recipes)
library(rsample)
library(parsnip)
library(workflows)
library(tune)
library(yardstick)
path <- "/Users/helena.schick/Documents/GitHub/r-sql-startup-success/df.csv"
df <- read_csv(path)
?estimate_tune_results
?estimate_tune_results
library(flexdashboard)
library(ggplot2)
library(plotly)
library(plyr)
library(readr)
library(leaflet)
library(visdat)
library(recipes)
library(rsample)
library(parsnip)
library(workflows)
library(tune)
library(yardstick)
path <- "/Users/helena.schick/Documents/GitHub/r-sql-startup-success/df.csv"
df <- read_csv(path)
p <- ggplot(df, aes(x = status, y = relationships, color = status)) +
geom_boxplot() +
theme_bw(base_size = 12) +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5))
ggplotly(p)
df$statuscol <- ifelse(df$status == "acquired", "red", "blue")
leaflet() %>%
addTiles() %>%
addCircleMarkers(df$longitude,
df$latitude,
radius = 6,
color = df$statuscol,
fill = df$statuscol,
popup = paste(df$state_code,
df$name,
df$status,
sep = "")) %>%
addLegend("bottomleft",
colors = c("red","blue"),
labels = c("acquired",
"closed"),
opacity = 0.8)
set.seed(42)
# Put 3/4 of the data into the training set
data_split <- initial_split(df,
prop = 3/4,
strata = status,
breaks = 4)
# Create dataframes for the two sets:
train_data <- training(data_split)
df_rec <-
recipe(status ~ ., data = train_data) %>%
step_rm(closed_at, state_code, category_code, latitude, longitude, zip_code, city, name, id) %>% #remove unnecessary columns
step_impute_median(age_first_milestone_year, age_last_milestone_year) %>% #replace NaN with median
step_naomit(everything(), skip = TRUE) %>% #remove rows with NaN values (only as double check)
step_mutate(founded_at = as.numeric(founded_at)) %>% #date to numeric
step_mutate(first_funding_at = as.numeric(first_funding_at)) %>%
step_mutate(last_funding_at = as.numeric(last_funding_at)) %>%
step_corr(all_predictors(), threshold = 0.7, method = "spearman") %>% #remove highly correlated
#step_outliers_maha(all_numeric(), -all_outcomes()) %>%
#step_outliers_lookout(all_numeric(),-contains(r"(.outliers)"),-all_outcomes()) %>%
#step_outliers_remove(contains(r"(.outliers)")) %>% # remove outliers
step_zv(all_numeric(), -all_outcomes()) %>% #remove all numeric that have zero variance
step_normalize(all_numeric(), -all_outcomes()) #standardize numeric features
#prepped_data <-
#  df_rec %>% # use the recipe object
#  prep() %>% # perform the recipe on training data
#  juice() # extract only the preprocessed dataframe
# crossvalidation
set.seed(100)
cv_folds <-
vfold_cv(train_data,
v = 5,
strata = status)
# Logistic Regression
# model
log_spec <- # your model specification
logistic_reg() %>%  # model type
set_engine(engine = "glm") %>%  # model engine
set_mode("classification") # model mode
# workflow pipeline
log_wflow <- # new workflow object
workflow() %>% # use workflow function
add_recipe(df_rec) %>%   # use the new recipe
add_model(log_spec)   # add your model spec
# fit model with crossvalidation
log_res <-
log_wflow %>%
fit_resamples(
resamples = cv_folds,
metrics = metric_set(yardstick::recall, yardstick::precision, accuracy),
control = control_resamples(save_pred = TRUE)
)
# Decision Tree
# model
xgb_spec <-
boost_tree() %>%
set_engine("xgboost") %>%
set_mode("classification")
# workflow pipeline
xgb_wflow <-
workflow() %>%
add_recipe(df_rec) %>%
add_model(xgb_spec)
# fit model with crossvalidation
xgb_res <-
xgb_wflow %>%
fit_resamples(
resamples = cv_folds,
metrics = metric_set(yardstick::recall, yardstick::precision, accuracy),
control = control_resamples(save_pred = TRUE)
)
# Tuned Decision Tree
# model
xgb_spec_tuned <-
boost_tree(trees = 1, min_n = 2, tree_depth = 1, learn_rate = 0.1, loss_reduction = 1e-10, stop_iter = 3) %>%
set_engine("xgboost") %>%
set_mode("classification")
# workflow pipeline
xgb_wflow_tuned <-
workflow() %>%
add_recipe(df_rec) %>%
add_model(xgb_spec_tuned)
# fit model with crossvalidation
xgb_res_tuned <-
xgb_wflow_tuned %>%
fit_resamples(
resamples = cv_folds,
metrics = metric_set(yardstick::recall, yardstick::precision, accuracy),
control = control_resamples(save_pred = TRUE)
)
# compare models
log_metrics <-
log_res %>%
collect_metrics(summarise = TRUE) %>%
plyr::mutate(model = "Logistic Regression") # add the name of the model to every row
set.seed(42)
# Put 3/4 of the data into the training set
data_split <- initial_split(df,
prop = 3/4,
strata = status,
breaks = 4)
# Create dataframes for the two sets:
train_data <- training(data_split)
df_rec <-
recipe(status ~ ., data = train_data) %>%
step_rm(closed_at, state_code, category_code, latitude, longitude, zip_code, city, name, id) %>% #remove unnecessary columns
step_impute_median(age_first_milestone_year, age_last_milestone_year) %>% #replace NaN with median
step_naomit(everything(), skip = TRUE) %>% #remove rows with NaN values (only as double check)
step_mutate(founded_at = as.numeric(founded_at)) %>% #date to numeric
step_mutate(first_funding_at = as.numeric(first_funding_at)) %>%
step_mutate(last_funding_at = as.numeric(last_funding_at)) %>%
step_corr(all_predictors(), threshold = 0.7, method = "spearman") %>% #remove highly correlated
#step_outliers_maha(all_numeric(), -all_outcomes()) %>%
#step_outliers_lookout(all_numeric(),-contains(r"(.outliers)"),-all_outcomes()) %>%
#step_outliers_remove(contains(r"(.outliers)")) %>% # remove outliers
step_zv(all_numeric(), -all_outcomes()) %>% #remove all numeric that have zero variance
step_normalize(all_numeric(), -all_outcomes()) #standardize numeric features
#prepped_data <-
#  df_rec %>% # use the recipe object
#  prep() %>% # perform the recipe on training data
#  juice() # extract only the preprocessed dataframe
# crossvalidation
set.seed(100)
cv_folds <-
vfold_cv(train_data,
v = 5,
strata = status)
# Logistic Regression
# model
log_spec <- # your model specification
logistic_reg() %>%  # model type
set_engine(engine = "glm") %>%  # model engine
set_mode("classification") # model mode
# workflow pipeline
log_wflow <- # new workflow object
workflow() %>% # use workflow function
add_recipe(df_rec) %>%   # use the new recipe
add_model(log_spec)   # add your model spec
# fit model with crossvalidation
log_res <-
log_wflow %>%
fit_resamples(
resamples = cv_folds,
metrics = metric_set(yardstick::recall, yardstick::precision, accuracy),
control = control_resamples(save_pred = TRUE)
)
# Decision Tree
# model
xgb_spec <-
boost_tree() %>%
set_engine("xgboost") %>%
set_mode("classification")
# workflow pipeline
xgb_wflow <-
workflow() %>%
add_recipe(df_rec) %>%
add_model(xgb_spec)
# fit model with crossvalidation
xgb_res <-
xgb_wflow %>%
fit_resamples(
resamples = cv_folds,
metrics = metric_set(yardstick::recall, yardstick::precision, accuracy),
control = control_resamples(save_pred = TRUE)
)
# Tuned Decision Tree
# model
xgb_spec_tuned <-
boost_tree(trees = 1, min_n = 2, tree_depth = 1, learn_rate = 0.1, loss_reduction = 1e-10, stop_iter = 3) %>%
set_engine("xgboost") %>%
set_mode("classification")
# workflow pipeline
xgb_wflow_tuned <-
workflow() %>%
add_recipe(df_rec) %>%
add_model(xgb_spec_tuned)
# fit model with crossvalidation
xgb_res_tuned <-
xgb_wflow_tuned %>%
fit_resamples(
resamples = cv_folds,
metrics = metric_set(yardstick::recall, yardstick::precision, accuracy),
control = control_resamples(save_pred = TRUE)
)
# compare models
log_metrics <-
log_res %>%
tune::collect_metrics(summarise = TRUE) %>%
plyr::mutate(model = "Logistic Regression") # add the name of the model to every row
show_notes(.Last.tune.result)
set.seed(42)
# Put 3/4 of the data into the training set
data_split <- initial_split(df,
prop = 3/4,
strata = status,
breaks = 4)
# Create dataframes for the two sets:
train_data <- training(data_split)
df_rec <-
recipe(status ~ ., data = train_data) %>%
step_rm(closed_at, state_code, category_code, latitude, longitude, zip_code, city, name, id) %>% #remove unnecessary columns
step_impute_median(age_first_milestone_year, age_last_milestone_year) %>% #replace NaN with median
step_naomit(everything(), skip = TRUE) %>% #remove rows with NaN values (only as double check)
step_mutate(founded_at = as.numeric(founded_at)) %>% #date to numeric
step_mutate(first_funding_at = as.numeric(first_funding_at)) %>%
step_mutate(last_funding_at = as.numeric(last_funding_at)) %>%
step_corr(all_predictors(), threshold = 0.7, method = "spearman") %>% #remove highly correlated
#step_outliers_maha(all_numeric(), -all_outcomes()) %>%
#step_outliers_lookout(all_numeric(),-contains(r"(.outliers)"),-all_outcomes()) %>%
#step_outliers_remove(contains(r"(.outliers)")) %>% # remove outliers
step_zv(all_numeric(), -all_outcomes()) %>% #remove all numeric that have zero variance
step_normalize(all_numeric(), -all_outcomes()) #standardize numeric features
prepped_data <-
df_rec %>% # use the recipe object
prep() %>% # perform the recipe on training data
juice() # extract only the preprocessed dataframe
View(df_rec)
View(df_rec_new)
set.seed(42)
# Put 3/4 of the data into the training set
data_split <- initial_split(df,
prop = 3/4,
strata = status,
breaks = 4)
# Create dataframes for the two sets:
train_data <- training(data_split)
df_rec <-
recipe(status ~ ., data = train_data) %>%
step_rm(closed_at, state_code, category_code, latitude, longitude, zip_code, city, name, id) %>% #remove unnecessary columns
step_impute_median(age_first_milestone_year, age_last_milestone_year) %>% #replace NaN with median
step_naomit(everything(), skip = TRUE) %>% #remove rows with NaN values (only as double check)
step_mutate(founded_at = as.numeric(founded_at)) %>% #date to numeric
step_mutate(first_funding_at = as.numeric(first_funding_at)) %>%
step_mutate(last_funding_at = as.numeric(last_funding_at)) %>%
step_corr(all_predictors(), threshold = 0.7, method = "spearman") %>% #remove highly correlated
#step_outliers_maha(all_numeric(), -all_outcomes()) %>%
#step_outliers_lookout(all_numeric(),-contains(r"(.outliers)"),-all_outcomes()) %>%
#step_outliers_remove(contains(r"(.outliers)")) %>% # remove outliers
step_zv(all_numeric(), -all_outcomes()) %>% #remove all numeric that have zero variance
step_normalize(all_numeric(), -all_outcomes()) #standardize numeric features
prepped_data <-
df_rec %>% # use the recipe object
prep() %>% # perform the recipe on training data
juice() # extract only the preprocessed dataframe
set.seed(42)
# Put 3/4 of the data into the training set
data_split <- initial_split(df,
prop = 3/4,
strata = status,
breaks = 4)
# Create dataframes for the two sets:
train_data <- training(data_split)
train_data <-
dplyr::select(-state_code, -category_code, -latitude, -longitude, -zip_code, -city, -name, -id)
set.seed(42)
# Put 3/4 of the data into the training set
data_split <- initial_split(df,
prop = 3/4,
strata = status,
breaks = 4)
# Create dataframes for the two sets:
train_data <- training(data_split)
train_data <-
plyr::select(-state_code, -category_code, -latitude, -longitude, -zip_code, -city, -name, -id)
library(flexdashboard)
library(ggplot2)
library(plotly)
library(plyr)
library(readr)
library(leaflet)
library(visdat)
library(recipes)
library(rsample)
library(parsnip)
library(workflows)
library(tune)
library(yardstick)
path <- "/Users/helena.schick/Documents/GitHub/r-sql-startup-success/df.csv"
df <- read_csv(path)
p <- ggplot(df, aes(x = status, y = relationships, color = status)) +
geom_boxplot() +
theme_bw(base_size = 12) +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5))
ggplotly(p)
df$statuscol <- ifelse(df$status == "acquired", "red", "blue")
leaflet() %>%
addTiles() %>%
addCircleMarkers(df$longitude,
df$latitude,
radius = 6,
color = df$statuscol,
fill = df$statuscol,
popup = paste(df$state_code,
df$name,
df$status,
sep = "")) %>%
addLegend("bottomleft",
colors = c("red","blue"),
labels = c("acquired",
"closed"),
opacity = 0.8)
set.seed(42)
# Put 3/4 of the data into the training set
data_split <- initial_split(df,
prop = 3/4,
strata = status,
breaks = 4)
# Create dataframes for the two sets:
train_data <- training(data_split)
train_data <-
dplyr::select(-state_code, -category_code, -latitude, -longitude, -zip_code, -city, -name, -id)
set.seed(42)
# Put 3/4 of the data into the training set
data_split <- initial_split(df,
prop = 3/4,
strata = status,
breaks = 4)
# Create dataframes for the two sets:
train_data <- training(data_split)
train_data <-
dplyr::select(-category_code, -latitude, -longitude, -zip_code, -city, -name, -id)
library(flexdashboard)
library(ggplot2)
library(plotly)
library(plyr)
library(readr)
library(leaflet)
library(visdat)
library(recipes)
library(rsample)
library(parsnip)
library(workflows)
library(tune)
library(yardstick)
path <- "/Users/helena.schick/Documents/GitHub/r-sql-startup-success/df.csv"
df <- read_csv(path)
p <- ggplot(df, aes(x = status, y = relationships, color = status)) +
geom_boxplot() +
theme_bw(base_size = 12) +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5))
ggplotly(p)
df$statuscol <- ifelse(df$status == "acquired", "red", "blue")
leaflet() %>%
addTiles() %>%
addCircleMarkers(df$longitude,
df$latitude,
radius = 6,
color = df$statuscol,
fill = df$statuscol,
popup = paste(df$state_code,
df$name,
df$status,
sep = "")) %>%
addLegend("bottomleft",
colors = c("red","blue"),
labels = c("acquired",
"closed"),
opacity = 0.8)
set.seed(42)
# Put 3/4 of the data into the training set
data_split <- initial_split(df,
prop = 3/4,
strata = status,
breaks = 4)
# Create dataframes for the two sets:
train_data <- training(data_split)
train_data <-
dplyr::select(-category_code, -latitude, -longitude, -zip_code, -city, -name, -id)
library(flexdashboard)
library(ggplot2)
library(plotly)
library(plyr)
library(readr)
library(leaflet)
library(visdat)
library(recipes)
library(rsample)
library(parsnip)
library(workflows)
library(tune)
library(yardstick)
path <- "/Users/helena.schick/Documents/GitHub/r-sql-startup-success/df.csv"
df <- read_csv(path)
p <- ggplot(df, aes(x = status, y = relationships, color = status)) +
geom_boxplot() +
theme_bw(base_size = 12) +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5))
ggplotly(p)
df$statuscol <- ifelse(df$status == "acquired", "red", "blue")
leaflet() %>%
addTiles() %>%
addCircleMarkers(df$longitude,
df$latitude,
radius = 6,
color = df$statuscol,
fill = df$statuscol,
popup = paste(df$state_code,
df$name,
df$status,
sep = "")) %>%
addLegend("bottomleft",
colors = c("red","blue"),
labels = c("acquired",
"closed"),
opacity = 0.8)
set.seed(42)
# Put 3/4 of the data into the training set
data_split <- initial_split(df,
prop = 3/4,
strata = status,
breaks = 4)
# Create dataframes for the two sets:
train_data <- training(data_split)
df_rec <-
recipe(status ~ ., data = train_data) %>%
step_rm(closed_at, state_code, category_code, latitude, longitude, zip_code, city, name, id) %>% #remove unnecessary columns
step_impute_median(age_first_milestone_year, age_last_milestone_year) %>% #replace NaN with median
step_naomit(everything(), skip = TRUE) %>% #remove rows with NaN values (only as double check)
step_mutate(founded_at = as.numeric(founded_at)) %>% #date to numeric
step_mutate(first_funding_at = as.numeric(first_funding_at)) %>%
step_mutate(last_funding_at = as.numeric(last_funding_at)) %>%
step_corr(all_predictors(), threshold = 0.7, method = "spearman") %>% #remove highly correlated
#step_outliers_maha(all_numeric(), -all_outcomes()) %>%
#step_outliers_lookout(all_numeric(),-contains(r"(.outliers)"),-all_outcomes()) %>%
#step_outliers_remove(contains(r"(.outliers)")) %>% # remove outliers
step_zv(all_numeric(), -all_outcomes()) %>% #remove all numeric that have zero variance
step_normalize(all_numeric(), -all_outcomes()) #standardize numeric features
prepped_data <-
df_rec %>% # use the recipe object
prep() %>% # perform the recipe on training data
juice() # extract only the preprocessed dataframe
?vip
??vip
?vip
?pluck
