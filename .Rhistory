}
#* Plot histogram
#* @png
#* @get /plot
function(){
hist(data_test$relationships)
}
pr$run(port=8000)
if (!require("plumber")) install.packages("plumber")
if (!require("jsonlite")) install.packages("jsonlite")
library(plumber)
library(jsonlite)
pr <- plumb("plumber-api.R")
# plumber.R
#* Plot histogram
#* @png
#* @get /plot
function(){
hist(data_test$relationships)
}
if (!require("plumber")) install.packages("plumber")
if (!require("jsonlite")) install.packages("jsonlite")
library(plumber)
library(jsonlite)
pr <- plumb("plumber-api.R")
pr <- plumb("plumber-api.R")
# plumber.R
#* Plot histogram
#* @png
#* @get /plot
function(){
hist(data_test$relationships)
}
pr <- plumb("plumber-api.R")
plumb(file='plumber.R')$run()
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(conflicted)
library(dplyr)
library(visdat)
library(rsample)
library(tidymodels)
library(skimr)
library(purrr)
library(GGally)
library(DBI)
library(maps)
library(mapproj)
library(caret)
library(xgboost)
library(kknn)
library(vip)
library(tidy.outliers)
path <- "/Users/helena.schick/Documents/GitHub/r-sql-startup-success/startup data.csv"
df <- read_csv(path)
glimpse(df)
vis_dat(df)
# convert all character variables to factors
df <-
df %>%
mutate(across(where(is.character), as.factor))
# convert all date variables to actual dates
df$founded_at <-
as.Date(df$founded_at, format = "%m/%d/%Y")
df$closed_at <-
as.Date(df$closed_at, format = "%m/%d/%Y")
df$first_funding_at <-
as.Date(df$first_funding_at, format = "%m/%d/%Y")
df$last_funding_at <-
as.Date(df$last_funding_at, format = "%m/%d/%Y")
# remove unnecessary variables
df <-
dplyr::select(df, -`Unnamed: 0`, -`Unnamed: 6`, -state_code.1, -object_id, -labels, -avg_participants)
# define outcome variable as y_label
y_label <- 'status'
# select feature names
features <-
df %>%
select(-all_of(y_label)) %>%
names()
# create feature data for data splitting
X <-
df %>%
select(all_of(features))
# list of numeric feature names
feat_num <-
X %>%
select(where(is.numeric)) %>%
names()
# list of categorical feature names
feat_cat <-
X %>%
select(!where(is.numeric)) %>%
names()
# create response for data splitting
y <-
df %>%
select(all_of(y_label))
# Fix the random numbers by setting the seed
# This enables the analysis to be reproducible
set.seed(42)
# Put 3/4 of the data into the training set
data_split <- initial_split(df,
prop = 3/4,
strata = status,
breaks = 4)
# Create dataframes for the two sets:
train_data <- training(data_split)
test_data <- testing(data_split)
df_train <- train_data
skim(df_train)
# Connection to database SQlite
con <- dbConnect(RSQLite::SQLite(), ":memory:")
# Write data "df_train" into database
dbWriteTable(con, "df_train", df_train)
# List tables
dbListTables(con)
n <- 10  # Number of most frequent levels to show
for (i in feat_cat){
counts <- df_train %>% count(!!sym(i)) %>% arrange(desc(n)) %>% head(n)
p <- ggplot(counts, aes_string(x=i, y="n")) +
geom_bar(stat="identity")
plot(p)
}
# Removing one hot encoded features
df_numeric <- df_train %>%
select(all_of(feat_num))
df_numeric <- df_numeric %>%
select(-is_CA, -is_NY, -is_MA, -is_TX, -is_otherstate, -is_software, -is_web, -is_mobile, -is_enterprise, -is_advertising, -is_gamesvideo, -is_ecommerce, -	is_biotech, -is_consulting, -is_enterprise, -is_othercategory, -has_VC, -has_angel, -has_roundA, -has_roundB, -has_roundC, -has_roundD, -is_top500)
# Create histograms using ggplot2
hist_list <- lapply(names(df_numeric), function(col) {
ggplot(df_numeric, aes(x = .data[[col]])) +
geom_histogram(bins = 10) +
ggtitle(paste0(col))
})
# Arrange histograms in a grid
library(patchwork)
hist_list[[1]] + hist_list[[2]] + hist_list[[3]] + plot_layout(ncol = 3)
hist_list[[4]] + hist_list[[5]] + hist_list[[6]] + plot_layout(ncol = 3)
hist_list[[7]] + hist_list[[8]] + hist_list[[9]] + plot_layout(ncol = 3)
df_numeric %>%
vis_cor(cor_method = "spearman", na_action = "pairwise.complete.obs")
df_numeric %>%
ggscatmat() +
theme(axis.text.x = element_text(angle = 90, vjust = -1))
df_train %>%
ggplot(aes(x = status, y = relationships, color = status)) +
geom_boxplot() +
labs(title = "Relationships, if successful or not") +
theme_bw(base_size = 12) +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5))
df_train %>%
ggplot(aes(x = relationships, y = funding_rounds, group = status, color = status)) +
geom_point() +
theme_classic(base_size = 12) +
ggtitle("Correlation of status, relationships and funding_rounds") +
theme(legend.title = element_blank())
world_map <- map_data("world")
ggplot(df_train, aes(x = longitude, y = latitude, color = status)) +
geom_point() +
geom_path(data = world_map, aes(x = long, y = lat, group = group), color = "gray50") +
labs(x = "Longitude", y = "Latitude", title = "Startup Locations")
library(ggmap)
qmplot(x = longitude,
y = latitude,
data = df_train,
geom = "point",
color = status,
alpha = 0.4) +
scale_alpha(guide = 'none') # don't show legend for alpha
df_train %>%
ggplot(aes(x = status, y = age_first_funding_year, color = status)) +
geom_boxplot() +
labs(title = "Age first funding, if successful or not") +
theme_bw(base_size = 12) +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5))
# remove duplicate rows
train_data <- distinct(train_data)
is.na(train_data) %>%
colSums()
# boxplot relationships
df_train %>%
ggplot(aes(y = relationships)) +
geom_boxplot()
# boxplot funding_total_usd
df_train %>%
ggplot(aes(y = funding_total_usd)) +
geom_boxplot()
# boxplot funding_rounds
df_train %>%
ggplot(aes(y = funding_rounds)) +
geom_boxplot()
# histogram founded_at
df_train %>%
ggplot(aes(x = founded_at)) +
geom_histogram()
df_rec <-
recipe(status ~ ., data = train_data) %>%
step_rm(closed_at, state_code, category_code, latitude, longitude, zip_code, city, name, id) %>% #remove unnecessary columns
step_impute_median(age_first_milestone_year, age_last_milestone_year) %>% #replace NaN with median
step_naomit(everything(), skip = TRUE) %>% #remove rows with NaN values (only as double check)
step_mutate(founded_at = as.numeric(founded_at)) %>% #date to numeric
step_mutate(first_funding_at = as.numeric(first_funding_at)) %>%
step_mutate(last_funding_at = as.numeric(last_funding_at)) %>%
step_corr(all_predictors(), threshold = 0.7, method = "spearman") %>% #remove highly correlated
#step_outliers_maha(all_numeric(), -all_outcomes()) %>%
#step_outliers_lookout(all_numeric(),-contains(r"(.outliers)"),-all_outcomes()) %>%
#step_outliers_remove(contains(r"(.outliers)")) %>% # remove outliers
step_zv(all_numeric(), -all_outcomes()) %>% #remove all numeric that have zero variance
step_normalize(all_numeric(), -all_outcomes()) #standardize numeric features
# bake recipe
prepped_data <-
df_rec %>% # use the recipe object
prep() %>% # perform the recipe on training data
juice() # extract only the preprocessed dataframe
# crossvalidation
set.seed(100)
cv_folds <-
vfold_cv(train_data,
v = 5,
strata = status)
# Logistic Regression
# model
log_spec <- # your model specification
logistic_reg() %>%  # model type
set_engine(engine = "glm") %>%  # model engine
set_mode("classification") # model mode
# workflow pipeline
log_wflow <- # new workflow object
workflow() %>% # use workflow function
add_recipe(df_rec) %>%   # use the new recipe
add_model(log_spec)   # add your model spec
# fit model with crossvalidation
log_res <-
log_wflow %>%
fit_resamples(
resamples = cv_folds,
metrics = metric_set(yardstick::recall, yardstick::precision, accuracy),
control = control_resamples(save_pred = TRUE)
)
# Decision Tree
# model
xgb_spec <-
boost_tree() %>%
set_engine("xgboost") %>%
set_mode("classification")
# workflow pipeline
xgb_wflow <-
workflow() %>%
add_recipe(df_rec) %>%
add_model(xgb_spec)
# fit model with crossvalidation
xgb_res <-
xgb_wflow %>%
fit_resamples(
resamples = cv_folds,
metrics = metric_set(yardstick::recall, yardstick::precision, accuracy),
control = control_resamples(save_pred = TRUE)
)
log_res %>%
collect_metrics(summarize = TRUE)
log_res %>%
collect_metrics(summarize = FALSE)
log_pred <-
log_res %>%
collect_predictions()
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(conflicted)
library(dplyr)
library(visdat)
library(rsample)
library(tidymodels)
library(skimr)
library(purrr)
library(GGally)
library(DBI)
library(maps)
library(mapproj)
library(caret)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(conflicted)
library(dplyr)
library(visdat)
library(rsample)
library(tidymodels)
library(skimr)
library(purrr)
library(GGally)
library(DBI)
library(maps)
library(mapproj)
library(caret)
library(xgboost)
library(kknn)
library(vip)
library(tidy.outliers)
path <- "/Users/helena.schick/Documents/GitHub/r-sql-startup-success/startup data.csv"
df <- read_csv(path)
glimpse(df)
vis_dat(df)
# convert all character variables to factors
df <-
df %>%
mutate(across(where(is.character), as.factor))
# convert all date variables to actual dates
df$founded_at <-
as.Date(df$founded_at, format = "%m/%d/%Y")
df$closed_at <-
as.Date(df$closed_at, format = "%m/%d/%Y")
df$first_funding_at <-
as.Date(df$first_funding_at, format = "%m/%d/%Y")
df$last_funding_at <-
as.Date(df$last_funding_at, format = "%m/%d/%Y")
# remove unnecessary variables
df <-
dplyr::select(df, -`Unnamed: 0`, -`Unnamed: 6`, -state_code.1, -object_id, -labels, -avg_participants)
# define outcome variable as y_label
y_label <- 'status'
# select feature names
features <-
df %>%
select(-all_of(y_label)) %>%
names()
# create feature data for data splitting
X <-
df %>%
select(all_of(features))
# list of numeric feature names
feat_num <-
X %>%
select(where(is.numeric)) %>%
names()
# list of categorical feature names
feat_cat <-
X %>%
select(!where(is.numeric)) %>%
names()
# create response for data splitting
y <-
df %>%
select(all_of(y_label))
# Fix the random numbers by setting the seed
# This enables the analysis to be reproducible
set.seed(42)
# Put 3/4 of the data into the training set
data_split <- initial_split(df,
prop = 3/4,
strata = status,
breaks = 4)
# Create dataframes for the two sets:
train_data <- training(data_split)
test_data <- testing(data_split)
df_train <- train_data
skim(df_train)
# Connection to database SQlite
con <- dbConnect(RSQLite::SQLite(), ":memory:")
# Write data "df_train" into database
dbWriteTable(con, "df_train", df_train)
# List tables
dbListTables(con)
n <- 10  # Number of most frequent levels to show
for (i in feat_cat){
counts <- df_train %>% count(!!sym(i)) %>% arrange(desc(n)) %>% head(n)
p <- ggplot(counts, aes_string(x=i, y="n")) +
geom_bar(stat="identity")
plot(p)
}
# Removing one hot encoded features
df_numeric <- df_train %>%
select(all_of(feat_num))
df_numeric <- df_numeric %>%
select(-is_CA, -is_NY, -is_MA, -is_TX, -is_otherstate, -is_software, -is_web, -is_mobile, -is_enterprise, -is_advertising, -is_gamesvideo, -is_ecommerce, -	is_biotech, -is_consulting, -is_enterprise, -is_othercategory, -has_VC, -has_angel, -has_roundA, -has_roundB, -has_roundC, -has_roundD, -is_top500)
# Create histograms using ggplot2
hist_list <- lapply(names(df_numeric), function(col) {
ggplot(df_numeric, aes(x = .data[[col]])) +
geom_histogram(bins = 10) +
ggtitle(paste0(col))
})
# Arrange histograms in a grid
library(patchwork)
hist_list[[1]] + hist_list[[2]] + hist_list[[3]] + plot_layout(ncol = 3)
hist_list[[4]] + hist_list[[5]] + hist_list[[6]] + plot_layout(ncol = 3)
hist_list[[7]] + hist_list[[8]] + hist_list[[9]] + plot_layout(ncol = 3)
df_numeric %>%
vis_cor(cor_method = "spearman", na_action = "pairwise.complete.obs")
df_numeric %>%
ggscatmat() +
theme(axis.text.x = element_text(angle = 90, vjust = -1))
df_train %>%
ggplot(aes(x = status, y = relationships, color = status)) +
geom_boxplot() +
labs(title = "Relationships, if successful or not") +
theme_bw(base_size = 12) +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5))
df_train %>%
ggplot(aes(x = relationships, y = funding_rounds, group = status, color = status)) +
geom_point() +
theme_classic(base_size = 12) +
ggtitle("Correlation of status, relationships and funding_rounds") +
theme(legend.title = element_blank())
world_map <- map_data("world")
ggplot(df_train, aes(x = longitude, y = latitude, color = status)) +
geom_point() +
geom_path(data = world_map, aes(x = long, y = lat, group = group), color = "gray50") +
labs(x = "Longitude", y = "Latitude", title = "Startup Locations")
library(ggmap)
qmplot(x = longitude,
y = latitude,
data = df_train,
geom = "point",
color = status,
alpha = 0.4) +
scale_alpha(guide = 'none') # don't show legend for alpha
df_train %>%
ggplot(aes(x = status, y = age_first_funding_year, color = status)) +
geom_boxplot() +
labs(title = "Age first funding, if successful or not") +
theme_bw(base_size = 12) +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5))
# remove duplicate rows
train_data <- distinct(train_data)
is.na(train_data) %>%
colSums()
# boxplot relationships
df_train %>%
ggplot(aes(y = relationships)) +
geom_boxplot()
# boxplot funding_total_usd
df_train %>%
ggplot(aes(y = funding_total_usd)) +
geom_boxplot()
# boxplot funding_rounds
df_train %>%
ggplot(aes(y = funding_rounds)) +
geom_boxplot()
# histogram founded_at
df_train %>%
ggplot(aes(x = founded_at)) +
geom_histogram()
df_rec <-
recipe(status ~ ., data = train_data) %>%
step_rm(closed_at, state_code, category_code, latitude, longitude, zip_code, city, name, id) %>% #remove unnecessary columns
step_impute_median(age_first_milestone_year, age_last_milestone_year) %>% #replace NaN with median
step_naomit(everything(), skip = TRUE) %>% #remove rows with NaN values (only as double check)
step_mutate(founded_at = as.numeric(founded_at)) %>% #date to numeric
step_mutate(first_funding_at = as.numeric(first_funding_at)) %>%
step_mutate(last_funding_at = as.numeric(last_funding_at)) %>%
step_corr(all_predictors(), threshold = 0.7, method = "spearman") %>% #remove highly correlated
#step_outliers_maha(all_numeric(), -all_outcomes()) %>%
#step_outliers_lookout(all_numeric(),-contains(r"(.outliers)"),-all_outcomes()) %>%
#step_outliers_remove(contains(r"(.outliers)")) %>% # remove outliers
step_zv(all_numeric(), -all_outcomes()) %>% #remove all numeric that have zero variance
step_normalize(all_numeric(), -all_outcomes()) #standardize numeric features
# bake recipe
prepped_data <-
df_rec %>% # use the recipe object
prep() %>% # perform the recipe on training data
juice() # extract only the preprocessed dataframe
# crossvalidation
set.seed(100)
cv_folds <-
vfold_cv(train_data,
v = 5,
strata = status)
# Logistic Regression
# model
log_spec <- # your model specification
logistic_reg() %>%  # model type
set_engine(engine = "glm") %>%  # model engine
set_mode("classification") # model mode
# workflow pipeline
log_wflow <- # new workflow object
workflow() %>% # use workflow function
add_recipe(df_rec) %>%   # use the new recipe
add_model(log_spec)   # add your model spec
# fit model with crossvalidation
log_res <-
log_wflow %>%
fit_resamples(
resamples = cv_folds,
metrics = metric_set(yardstick::recall, yardstick::precision, accuracy),
control = control_resamples(save_pred = TRUE)
)
# Decision Tree
# model
xgb_spec <-
boost_tree() %>%
set_engine("xgboost") %>%
set_mode("classification")
# workflow pipeline
xgb_wflow <-
workflow() %>%
add_recipe(df_rec) %>%
add_model(xgb_spec)
# fit model with crossvalidation
xgb_res <-
xgb_wflow %>%
fit_resamples(
resamples = cv_folds,
metrics = metric_set(yardstick::recall, yardstick::precision, accuracy),
control = control_resamples(save_pred = TRUE)
)
log_res %>%
collect_metrics(summarize = TRUE)
log_res %>%
collect_metrics(summarize = FALSE)
log_pred <-
log_res %>%
collect_predictions()
plumber::plumb(file='plumber.R')$run()
